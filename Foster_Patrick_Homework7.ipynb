{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3827992f-9986-4ff3-8291-8d1c9d165dc2",
   "metadata": {},
   "source": [
    "## Homework 7\n",
    "\n",
    "For this week's homework, you will get more experience with LoRA. Like we did in class, you will choose a training dataset and a general language benchmark to assess catastrophic forgetting (you must use the same datasets you used last week for prompt tuning so that you can directly compare the results from both methods). You will use `lm_eval` to assess your model's performance before and after LoRA. The goal of this assignment is to get more experience implementing LoRA and compare its effectiveness to lighter approaches like prompt tuning. This homework will guide you through the steps to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043ec71-9bfb-474b-90eb-b09b52d65671",
   "metadata": {},
   "source": [
    "## Step 1: Download a Model and Assess Its Pretraining Performance (20 points)\n",
    "As we did in class, start by downloading a model and assessing its pretraining performance on your training task and a general language benchmark using the `lm_eval` package (if your dataset is not a `lm_eval` task, you will need to write your own code to generate responses to a hold-out test split of the dataset and assess whether they are correct or not). Here are step-by-step directions with point values:\n",
    "- Import necessary dependencies (5 points)\n",
    "- Load the model and tokenizer (5 points) \n",
    "- Set up a `lm_eval` `task_manager` and implement your training task and general language benchmark; make sure to log the samples and you can set a limit (n=50) if you'd like to reduce runtime. If you are implementing a training task not in `lm_eval`, load the necessary data, conduct a training and test split (using the same `seed` as HW 9), and evaluate your model on the test dataset, logging responses and using a limit if you'd like (5 points).\n",
    "- Print the results and 2 model responses for each task to get a sense of what the model outputs look like before training (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8d7c12-606c-4c5e-9966-1bf2a61b6dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfcc811c7bb487c8e55fc97f18eafac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"HF_HOME\"] = \"/scratch/ezq9qu/models/cache\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from tqdm import tqdm\n",
    "from lm_eval import evaluator\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\", device_map = 'auto', dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fbb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3acccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 50/50 [00:00<00:00, 4452.84it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 3296.42it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 400/400 [00:17<00:00, 22.87it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = \"hf\", #Specify huggingface model\n",
    "    model_args = {\"pretrained\": model, \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer}, #Define model arguments\n",
    "    tasks = [\"hellaswag\", \"race\"], \n",
    "    log_samples = True, \n",
    "    batch_size = \"1\",\n",
    "    limit = 50,\n",
    "    random_seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca242ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.4,\n",
       "  'acc_stderr,none': 0.06998542122237653,\n",
       "  'acc_norm,none': 0.46,\n",
       "  'acc_norm_stderr,none': 0.07119963311072637},\n",
       " 'race': {'alias': 'race',\n",
       "  'acc,none': 0.36,\n",
       "  'acc_stderr,none': 0.06857142857142856}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d893cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hella_examples(index):\n",
    "    query = results[\"samples\"][\"hellaswag\"][index][\"doc\"]['query']\n",
    "    choices = results[\"samples\"][\"hellaswag\"][index][\"doc\"][\"choices\"]\n",
    "    target = results[\"samples\"][\"hellaswag\"][index]['target']\n",
    "    resps = max(enumerate(results[\"samples\"][\"hellaswag\"][index][\"resps\"]), key=lambda x: x[1][0])[0]\n",
    "\n",
    "    print(f\"Query: {query}: \\n\")\n",
    "    for choice in choices:\n",
    "        print(f\"{choice}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Response: {resps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6f79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_examples(index):\n",
    "    article = results[\"samples\"][\"race\"][index]['doc']['article']\n",
    "    article_wrapped = textwrap.fill(article, width=100)\n",
    "    problems = results[\"samples\"][\"race\"][index]['doc']['problems']\n",
    "\n",
    "    data_list = ast.literal_eval(problems)\n",
    "    questions = [item['question'] for item in data_list]\n",
    "\n",
    "    target = results[\"samples\"][\"race\"][index][\"target\"]\n",
    "    resps = max(enumerate(results[\"samples\"][\"race\"][index][\"resps\"]), key=lambda x: x[1][0])[0]\n",
    "\n",
    "    print(f\"Article: {article_wrapped} \\n\\n \")\n",
    "    print(\"There are a series of questions...\")\n",
    "    for question in questions:\n",
    "        print(question)\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Resps: {resps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892bd261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Roof shingle removal: A man is sitting on a roof. He: \n",
      "\n",
      "is using wrap to wrap a pair of skis.\n",
      "is ripping level tiles off.\n",
      "is holding a rubik's cube.\n",
      "starts pulling up roofing on a roof.\n",
      "\n",
      "\n",
      "Target: 3\n",
      "Response: 2\n"
     ]
    }
   ],
   "source": [
    "get_hella_examples(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e47616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady: \n",
      "\n",
      "swings and lands in her arms.\n",
      "pulls the barbell forward.\n",
      "pulls a rope attached to the barbell.\n",
      "stands and lifts the weight over her head.\n",
      "\n",
      "\n",
      "Target: 3\n",
      "Response: 1\n"
     ]
    }
   ],
   "source": [
    "get_hella_examples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc6aa308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: The rain had continued for a week and the flood had created a big river which were running by Nancy\n",
      "Brown's farm. As she tried to gather her cows to a higher ground, she slipped and hit her head on a\n",
      "fallen tree trunk. The fall made her unconscious for a moment or two. When she came to, Lizzie, one\n",
      "of her oldest and favorite cows, was licking her face.  At that time, the water level on the farm\n",
      "was still rising. Nancy gathered all her strength to get up and began walking slowly with Lizzie.\n",
      "The rain had become much heavier, and the water in the field was now waist high. Nancy's pace got\n",
      "slower and slower because she felt a great pain in her head. Finally, all she could do was to throw\n",
      "her arm around Lizzie's neck and try to hang on. About 20 minutes later, Lizzie managed to pull\n",
      "herself and Nancy out of the rising water and onto a bit of high land, which seemed like a small\n",
      "island in the middle of a lake of white water.  Even though it was about noon, the sky was so dark\n",
      "and the rain and lightning was so bad that it took rescuers more than two hours to discover Nancy. A\n",
      "man from a helicopter  lowered a rope, but Nancy couldn't catch it. A moment later, two men landed\n",
      "on the small island from a ladder in the helicopter. They raised her into the helicopter and took\n",
      "her to the school gym, where the Red Cross had set up an emergency shelter.  When the flood\n",
      "disappeared two days later, Nancy immediately went back to the \"island.\" Lizzie was gone. She was\n",
      "one of 19 cows that Nancy had lost in the flood. \"I owe my life to her,\" said Nancy with tears. \n",
      "\n",
      " \n",
      "There are a series of questions...\n",
      "What did Nancy try to do before she fell over?\n",
      "The following are true according to the passage except  _  .\n",
      "What did the local people do to help those in the flooded area according to the passage?\n",
      "Target: 0\n",
      "Resps: 3\n"
     ]
    }
   ],
   "source": [
    "get_race_examples(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7fd52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: There is probably no field of human activity in which our values and lifestyles are shown more\n",
      "clearly and strongly than they are in the clothes that we choose to wear.The dress of an individual\n",
      "is a kind of \"sign language\" that communicates a set of information and is usually the basis on\n",
      "which immediate impressions are formed.Traditionally,a concern for clothes was considered to be an\n",
      "affair of females,while men took pride in the fact that they were completely lacking in clothes\n",
      "consciousness . This type of American culture is by degrees changing as man dress takes on greater\n",
      "variety and color.Even as early as 1955,a researcher in Michigan said that _ White collar workers in\n",
      "particular viewed dress as a symbol of ability,which could be used to impress or influence\n",
      "others,especially in the work situation.The white collar worker was described as extremely concerned\n",
      "about the impression his clothing made on his superiors .Although blue collar workers were less\n",
      "aware that they might be judged on the basis of their clothing,they recognized that any difference\n",
      "from the accepted pattern of dress would be made fun of by fellow workers. Since that time,of\n",
      "course,the patterns have changed:the typical office worker may now be wearing the blue shirt,and the\n",
      "laborer a white shirt;but the importance of dress has not become less.Other researchers in recent\n",
      "years have helped to prove its importance in the lives of individuals at various age levels and in\n",
      "different social and economic status groups . \n",
      "\n",
      " \n",
      "There are a series of questions...\n",
      "The passage tells us that   _  .\n",
      "Traditionally,people usually thought that   _  .\n",
      "Blue collar workers pay attention to their clothes because   _  .\n",
      "The passage mainly suggests that   _  .\n",
      "Target: 2\n",
      "Resps: 3\n"
     ]
    }
   ],
   "source": [
    "get_race_examples(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ece590-82e7-43d0-970e-b7110d912a4e",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data and Model for LoRA (20 points)\n",
    "Step-by-step directions:\n",
    "- Make sure the tokenizer and model have a `pad` token set (2 points)\n",
    "- Load your training dataset and do a training/validation split (using the same `seed` as HW 9), ensuring that your data are formatted in an instruction/response format (i.e., make sure each row denotes the instruction/question and response separately; for example, `\"Instruct: {instruction}\\n\\nResponse: {response}\"`). If you are using a `lm_eval` task for your training task, make sure your training/validation data are from the training split of the benchmark so that you are not directly training on test benchmark data. If you are not using a `lm_eval` task for your training task, make sure your training dataset does not contain examples from the test split you used above and that your evaluation dataset comes from a split of your training dataset so that you are not tuning performance on the test dataset directly. (10 points)\n",
    "- Tokenize your training and validation data using the map function (2 points)\n",
    "- Print 1 training and 1 validation data sample to make sure the mapping worked and the data are formatted properly (2 points)\n",
    "- Set the number of training epochs (1-2 training epochs is fine) (2 points)\n",
    "- Set up a `LoraConfig` using your desired hyperparameters (including `lora_alpha`, `lora_dropout`, `bias`, `task_type`, and `target_modules`) and print the number of trainable parameters (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6206f2fc-fe8b-4016-9824-9b2e8936585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_virtual_tokens = 10\n",
    "num_epochs = 2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636570fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"hellaswag\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c0368e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:2000]\n",
    "val = data[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "364366f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(train)\n",
    "val_dataset = Dataset.from_dict(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc5bae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(example):\n",
    "    instruction = example['ctx']\n",
    "    correct_ending_index = int(example['label'])\n",
    "    response = example['endings'][correct_ending_index]\n",
    "\n",
    "    text = f\"Instruct: Complete the sentence: {instruction}\\n\\nResponse: {response}\"\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94257784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "334a5ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e91532b2e864c5ebc65dedd349ff5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7af0bba3ae647e9b5fd3c632140b1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_train_data = train_dataset.map(format_data, remove_columns=list(train_dataset.features))\n",
    "formatted_val_data = val_dataset.map(format_data, remove_columns=list(val_dataset.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "498b13ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e617b31197d941bfb7f15f1c939820ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca9b92d412e4503b80e59f68481c91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = formatted_train_data.map(tokenize, batched=True, remove_columns=['text'])\n",
    "tokenized_val = formatted_val_data.map(tokenize, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2ff7de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [641,\n",
       "  1235,\n",
       "  25,\n",
       "  18608,\n",
       "  279,\n",
       "  11652,\n",
       "  25,\n",
       "  5005,\n",
       "  11,\n",
       "  279,\n",
       "  883,\n",
       "  13914,\n",
       "  916,\n",
       "  279,\n",
       "  11794,\n",
       "  18202,\n",
       "  279,\n",
       "  3241,\n",
       "  315,\n",
       "  264,\n",
       "  1803,\n",
       "  11,\n",
       "  323,\n",
       "  264,\n",
       "  5220,\n",
       "  12233,\n",
       "  12406,\n",
       "  15097,\n",
       "  42532,\n",
       "  13,\n",
       "  1221,\n",
       "  271,\n",
       "  2582,\n",
       "  25,\n",
       "  1154,\n",
       "  279,\n",
       "  883,\n",
       "  9539,\n",
       "  17592,\n",
       "  279,\n",
       "  11794,\n",
       "  389,\n",
       "  806,\n",
       "  1803,\n",
       "  13],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cf6901f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [641,\n",
       "  1235,\n",
       "  25,\n",
       "  18608,\n",
       "  279,\n",
       "  11652,\n",
       "  25,\n",
       "  362,\n",
       "  5220,\n",
       "  374,\n",
       "  6839,\n",
       "  48348,\n",
       "  2348,\n",
       "  264,\n",
       "  7002,\n",
       "  13,\n",
       "  25694,\n",
       "  4935,\n",
       "  261,\n",
       "  20114,\n",
       "  525,\n",
       "  6839,\n",
       "  304,\n",
       "  264,\n",
       "  3054,\n",
       "  13,\n",
       "  2155,\n",
       "  12538,\n",
       "  271,\n",
       "  2582,\n",
       "  25,\n",
       "  525,\n",
       "  20459,\n",
       "  553,\n",
       "  1105,\n",
       "  13],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aefa3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,845,056 || all params: 1,733,420,032 || trainable%: 0.7410\n"
     ]
    }
   ],
   "source": [
    "LORA_R = 64 \n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = .05\n",
    "lora_config = LoraConfig(\n",
    "    r = LORA_R, #the lower dimension of the low-rank matrices\n",
    "    lora_alpha = LORA_ALPHA, #scaling factor for the low-rank update\n",
    "    lora_dropout = LORA_DROPOUT, #dropout factor to prevent overfitting\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\", #set language modeling as task type\n",
    "    target_modules = [\"q_proj\", \"v_proj\"], #add LoRA modules to every query and value matrix in the attention layers\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947c49f-7ac9-4aef-a26f-a3eec5a2e935",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Trainer and Train Model (20 points)\n",
    "Step-by-step directions:\n",
    "- As we did in class, create `TrainingArguments` that define an output directory (make sure the directory exists like we did in class); batch size; learning rate; number of training epochs; `logging_steps`, `eval_strategy`, and `eval_steps` for evaluating on the training and validation dataset; set the number of `save_steps`; and set `load_best_model_at_end` to `True` (5 points)\n",
    "- Create a trainer using your training arguments and defining the model, `train_dataset`, `eval_dataset`, and `data_collator` (5 points)\n",
    "- Run `trainer.train()` to train your model (5 points)\n",
    "- Save your model after training in a folder in your output_directory called best_model since the trainer will automatically load your best model at the end of training (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e3d237-23b9-4170-b33e-ef95ab7d5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "def create_training_arguments(path, learning_rate = 0.00001, epochs = 1, eval_steps = 150):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, #specify path for trained model weights\n",
    "        auto_find_batch_size = True, #automatically find batch size\n",
    "        learning_rate = learning_rate,\n",
    "        num_train_epochs = epochs, \n",
    "        logging_steps = eval_steps, #this is how often we log training results\n",
    "        eval_strategy = \"steps\", #evaluate every 150 steps\n",
    "        eval_steps = eval_steps,\n",
    "        save_steps = eval_steps,\n",
    "        load_best_model_at_end = True,\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "513ad91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/scratch/ezq9qu\"\n",
    "output_directory = os.path.join(working_dir, \"trained_lora_model\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4c0208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = create_training_arguments(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ef84302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "def create_trainer(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                        mlm= False),\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46cec447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(model, training_args, tokenized_train, tokenized_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e5cd749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpfoster317\u001b[0m (\u001b[33mpfoster317-university-of-virginia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sfs/gpfs/tardis/home/ezq9qu/lm-evaluation-harness/wandb/run-20251104_212513-5otvtq6m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pfoster317-university-of-virginia/huggingface/runs/5otvtq6m' target=\"_blank\">bright-planet-5</a></strong> to <a href='https://wandb.ai/pfoster317-university-of-virginia/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pfoster317-university-of-virginia/huggingface' target=\"_blank\">https://wandb.ai/pfoster317-university-of-virginia/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pfoster317-university-of-virginia/huggingface/runs/5otvtq6m' target=\"_blank\">https://wandb.ai/pfoster317-university-of-virginia/huggingface/runs/5otvtq6m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.285000</td>\n",
       "      <td>2.753965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=3.05614794921875, metrics={'train_runtime': 54.5606, 'train_samples_per_second': 36.656, 'train_steps_per_second': 4.582, 'total_flos': 1184522318856192.0, 'train_loss': 3.05614794921875, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e25ab45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(f\"{output_directory}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593bc30-ab22-4618-b274-02db52a79d99",
   "metadata": {},
   "source": [
    "## Step 4: Assess Post-training Performance (20 points)\n",
    "Step-by-step directions:\n",
    "- Assess post training performance: Set up an `lm_eval` `task_manager` and implement your training task's test split and general language benchmark; make sure to log the samples and you can set a limit (n=50) if you'd like. If you are implementing a training task not in `lm_eval`, remember to use your held-out test data, logging responses and using a limit if you'd like. (10 points)\n",
    "- Print the results and 2 model responses for each task to get a sense of what the model outputs look like after training (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29bd05bd-1f1e-4b4c-be19-51349f8057d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 50/50 [00:00<00:00, 4476.79it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 3361.57it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 400/400 [00:20<00:00, 19.29it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = \"hf\", #Specify huggingface model\n",
    "    model_args = {\"pretrained\": model, \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer}, #Define model arguments\n",
    "    tasks = [\"hellaswag\", \"race\"], \n",
    "    log_samples = True, \n",
    "    batch_size = \"1\",\n",
    "    limit = 50,\n",
    "    random_seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69d8b2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.46,\n",
       "  'acc_stderr,none': 0.07119963311072637,\n",
       "  'acc_norm,none': 0.5,\n",
       "  'acc_norm_stderr,none': 0.07142857142857142},\n",
       " 'race': {'alias': 'race',\n",
       "  'acc,none': 0.34,\n",
       "  'acc_stderr,none': 0.0676726816132972}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb124927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Roof shingle removal: A man is sitting on a roof. He: \n",
      "\n",
      "is using wrap to wrap a pair of skis.\n",
      "is ripping level tiles off.\n",
      "is holding a rubik's cube.\n",
      "starts pulling up roofing on a roof.\n",
      "\n",
      "\n",
      "Target: 3\n",
      "Response: 2\n"
     ]
    }
   ],
   "source": [
    "get_hella_examples(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7285b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady: \n",
      "\n",
      "swings and lands in her arms.\n",
      "pulls the barbell forward.\n",
      "pulls a rope attached to the barbell.\n",
      "stands and lifts the weight over her head.\n",
      "\n",
      "\n",
      "Target: 3\n",
      "Response: 1\n"
     ]
    }
   ],
   "source": [
    "get_hella_examples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3d6592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: The rain had continued for a week and the flood had created a big river which were running by Nancy\n",
      "Brown's farm. As she tried to gather her cows to a higher ground, she slipped and hit her head on a\n",
      "fallen tree trunk. The fall made her unconscious for a moment or two. When she came to, Lizzie, one\n",
      "of her oldest and favorite cows, was licking her face.  At that time, the water level on the farm\n",
      "was still rising. Nancy gathered all her strength to get up and began walking slowly with Lizzie.\n",
      "The rain had become much heavier, and the water in the field was now waist high. Nancy's pace got\n",
      "slower and slower because she felt a great pain in her head. Finally, all she could do was to throw\n",
      "her arm around Lizzie's neck and try to hang on. About 20 minutes later, Lizzie managed to pull\n",
      "herself and Nancy out of the rising water and onto a bit of high land, which seemed like a small\n",
      "island in the middle of a lake of white water.  Even though it was about noon, the sky was so dark\n",
      "and the rain and lightning was so bad that it took rescuers more than two hours to discover Nancy. A\n",
      "man from a helicopter  lowered a rope, but Nancy couldn't catch it. A moment later, two men landed\n",
      "on the small island from a ladder in the helicopter. They raised her into the helicopter and took\n",
      "her to the school gym, where the Red Cross had set up an emergency shelter.  When the flood\n",
      "disappeared two days later, Nancy immediately went back to the \"island.\" Lizzie was gone. She was\n",
      "one of 19 cows that Nancy had lost in the flood. \"I owe my life to her,\" said Nancy with tears. \n",
      "\n",
      " \n",
      "There are a series of questions...\n",
      "What did Nancy try to do before she fell over?\n",
      "The following are true according to the passage except  _  .\n",
      "What did the local people do to help those in the flooded area according to the passage?\n",
      "Target: 0\n",
      "Resps: 3\n"
     ]
    }
   ],
   "source": [
    "get_race_examples(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aafed8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: There is probably no field of human activity in which our values and lifestyles are shown more\n",
      "clearly and strongly than they are in the clothes that we choose to wear.The dress of an individual\n",
      "is a kind of \"sign language\" that communicates a set of information and is usually the basis on\n",
      "which immediate impressions are formed.Traditionally,a concern for clothes was considered to be an\n",
      "affair of females,while men took pride in the fact that they were completely lacking in clothes\n",
      "consciousness . This type of American culture is by degrees changing as man dress takes on greater\n",
      "variety and color.Even as early as 1955,a researcher in Michigan said that _ White collar workers in\n",
      "particular viewed dress as a symbol of ability,which could be used to impress or influence\n",
      "others,especially in the work situation.The white collar worker was described as extremely concerned\n",
      "about the impression his clothing made on his superiors .Although blue collar workers were less\n",
      "aware that they might be judged on the basis of their clothing,they recognized that any difference\n",
      "from the accepted pattern of dress would be made fun of by fellow workers. Since that time,of\n",
      "course,the patterns have changed:the typical office worker may now be wearing the blue shirt,and the\n",
      "laborer a white shirt;but the importance of dress has not become less.Other researchers in recent\n",
      "years have helped to prove its importance in the lives of individuals at various age levels and in\n",
      "different social and economic status groups . \n",
      "\n",
      " \n",
      "There are a series of questions...\n",
      "The passage tells us that   _  .\n",
      "Traditionally,people usually thought that   _  .\n",
      "Blue collar workers pay attention to their clothes because   _  .\n",
      "The passage mainly suggests that   _  .\n",
      "Target: 2\n",
      "Resps: 3\n"
     ]
    }
   ],
   "source": [
    "get_race_examples(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa5c4e-447a-4a99-abe3-d44fead029eb",
   "metadata": {},
   "source": [
    "## Step 5: Interpretation and Comparison to Prompt Tuning (20 points)\n",
    "In one-two paragraphs, interpret the results from LoRA. Discuss whether you think LoRA worked well for your task, citing quantitative and qualitative evidence from Steps 1 and 4 (6 points). Also discuss why you think LoRA performed the way it did for your task (i.e., why did it succeed or fail?), citing any resources you used to formulate your argument (6 points). Lastly, compare LoRA's performance on your task to prompt tuning: which method worked better in your opinion, and why (8 points)? Some considerations to help you make your decision include accuracy on the test set, difference in outputs before and after training, and level of catastrophic forgetting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2051c267",
   "metadata": {},
   "source": [
    "Based on the quantitative results, LoRA training worked fairly well, the acc,none for the task increased from .40 to .46, the other benchmark that was used race did not change much either from .36 to .34. There was not much catastrophic forgetting. The responses for the qualatative task did not give much information, the models made the same mistakes for both selected answers. I think the Lora succeeded as it allowed for the model to learn the specifics for the inputs needed for this spefic test. Interestingly this was achieved with less training data than the prompt tuning task. I think the LoRA model was more successful in this case than the prompt tuning. LoRA achieved a better score on the hellaswag, saw a similar amount of forgetting, and did it with less resources. This is much more efficient than the prompt tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549d1bc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
