{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3827992f-9986-4ff3-8291-8d1c9d165dc2",
   "metadata": {},
   "source": [
    "## Week 9 Homework\n",
    "\n",
    "For this week's homework, you will get more experience with prompt tuning. Like we did in class, you will choose a training dataset and a general language benchmark to assess catastrophic forgetting (I recommend using datasets you will use for your final project to practice implementing them). You will use `lm_eval` to assess your model's performance before and after prompt tuning. Like in class, it is okay if prompt tuning does not work well for your task- the goal of this assignment is to get more experience with implementation and further understand what tasks prompt tuning works well for and poorly for. This homework will guide you through the steps to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043ec71-9bfb-474b-90eb-b09b52d65671",
   "metadata": {},
   "source": [
    "## Step 1: Download a Model and Assess Its Pretraining Performance (20 points)\n",
    "As we did in class, start by downloading a model and assessing its pretraining performance on your training task and a general language benchmark using the `lm_eval` package (if your dataset is not a `lm_eval` task, you will need to write your own code to generate responses to a hold-out test split of the dataset and assess whether they are correct or not). Here are step-by-step directions with point values:\n",
    "- Import necessary dependencies (5 points)\n",
    "- Load the model and tokenizer (5 points) \n",
    "- Set up a `lm_eval` `task_manager` and implement your training task and general language benchmark; make sure to log the samples and you can set a limit (n=50) if you'd like to reduce runtime. If you are implementing a training task not in `lm_eval`, load the necessary data, conduct a training and test split (use a `seed` so you can reproduce the split in future homeworks and your final assignment), and evaluate your model on the test dataset, logging responses and using a limit if you'd like (5 points).\n",
    "- Print the results and 2 model responses for each task to get a sense of what the model outputs look like before training (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8d7c12-606c-4c5e-9966-1bf2a61b6dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2341c444b70416783b73e81484ca2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"HF_HOME\"] = \"/scratch/ezq9qu/models/cache\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from lm_eval import evaluator\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\", device_map = 'auto', dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcff3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 50/50 [00:00<00:00, 4678.95it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 3503.66it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 400/400 [00:17<00:00, 22.98it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = \"hf\", #Specify huggingface model\n",
    "    model_args = {\"pretrained\": model, \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer}, #Define model arguments\n",
    "    tasks = [\"hellaswag\", \"race\"], \n",
    "    log_samples = True, \n",
    "    batch_size = \"1\",\n",
    "    limit = 50,\n",
    "    random_seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5845ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.4,\n",
       "  'acc_stderr,none': 0.06998542122237653,\n",
       "  'acc_norm,none': 0.46,\n",
       "  'acc_norm_stderr,none': 0.07119963311072637},\n",
       " 'race': {'alias': 'race',\n",
       "  'acc,none': 0.36,\n",
       "  'acc_stderr,none': 0.06857142857142856}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfb1babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hella_examples(index):\n",
    "    query = results[\"samples\"][\"hellaswag\"][index][\"doc\"]['query']\n",
    "    choices = results[\"samples\"][\"hellaswag\"][index][\"doc\"][\"choices\"]\n",
    "    target = results[\"samples\"][\"hellaswag\"][index]['target']\n",
    "    resps = max(enumerate(results[\"samples\"][\"hellaswag\"][index][\"resps\"]), key=lambda x: x[1][0])[0]\n",
    "\n",
    "    print(f\"Query: {query}: \\n\")\n",
    "    for choice in choices:\n",
    "        print(f\"{choice}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Response: {resps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31351d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Roof shingle removal: A man is sitting on a roof. He: \n",
      "\n",
      "is using wrap to wrap a pair of skis.\n",
      "is ripping level tiles off.\n",
      "is holding a rubik's cube.\n",
      "starts pulling up roofing on a roof.\n",
      "\n",
      "\n",
      "Target: 3\n",
      "Response: 2\n"
     ]
    }
   ],
   "source": [
    "get_hella_examples(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "011f39ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady: \n",
      "\n",
      "swings and lands in her arms.\n",
      "pulls the barbell forward.\n",
      "pulls a rope attached to the barbell.\n",
      "stands and lifts the weight over her head.\n",
      "\n",
      "\n",
      "Target: 3\n",
      "Response: 1\n"
     ]
    }
   ],
   "source": [
    "get_hella_examples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27f879b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1ac619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'question': 'What did Nancy try to do before she fell over?', 'answer': 'C', 'options': ['Measure the depth of the river', 'Look for a fallen tree trunk', 'Protect her cows from being drowned', 'Run away from the flooded farm']}, {'question': 'The following are true according to the passage except  _  .', 'answer': 'D', 'options': ['It took Lizzie and Nancy about 20 minutes to get to safety.', 'It was raining harder when Nancy managed to get up.', 'The bad weather made it difficult for rescuers to find Nancy.', 'Nancy took hold of the rope and climbed into the helicopter.']}, {'question': 'What did the local people do to help those in the flooded area according to the passage?', 'answer': 'A', 'options': ['They put up shelter for them in a school.', 'They used helicopters to help carry cows.', 'They helped farmers gather their cows.', 'They set up an organization called Red Cross.']}]\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"samples\"][\"race\"][0]['doc']['problems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "675fc5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_examples(index):\n",
    "    article = results[\"samples\"][\"race\"][index]['doc']['article']\n",
    "    article_wrapped = textwrap.fill(article, width=100)\n",
    "    problems = results[\"samples\"][\"race\"][index]['doc']['problems']\n",
    "\n",
    "    data_list = ast.literal_eval(problems)\n",
    "    questions = [item['question'] for item in data_list]\n",
    "\n",
    "    target = results[\"samples\"][\"race\"][index][\"target\"]\n",
    "    resps = max(enumerate(results[\"samples\"][\"race\"][index][\"resps\"]), key=lambda x: x[1][0])[0]\n",
    "\n",
    "    print(f\"Article: {article_wrapped} \\n\\n \")\n",
    "    print(\"There are a series of questions...\")\n",
    "    for question in questions:\n",
    "        print(question)\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Resps: {resps}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6fc850a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: The rain had continued for a week and the flood had created a big river which were running by Nancy\n",
      "Brown's farm. As she tried to gather her cows to a higher ground, she slipped and hit her head on a\n",
      "fallen tree trunk. The fall made her unconscious for a moment or two. When she came to, Lizzie, one\n",
      "of her oldest and favorite cows, was licking her face.  At that time, the water level on the farm\n",
      "was still rising. Nancy gathered all her strength to get up and began walking slowly with Lizzie.\n",
      "The rain had become much heavier, and the water in the field was now waist high. Nancy's pace got\n",
      "slower and slower because she felt a great pain in her head. Finally, all she could do was to throw\n",
      "her arm around Lizzie's neck and try to hang on. About 20 minutes later, Lizzie managed to pull\n",
      "herself and Nancy out of the rising water and onto a bit of high land, which seemed like a small\n",
      "island in the middle of a lake of white water.  Even though it was about noon, the sky was so dark\n",
      "and the rain and lightning was so bad that it took rescuers more than two hours to discover Nancy. A\n",
      "man from a helicopter  lowered a rope, but Nancy couldn't catch it. A moment later, two men landed\n",
      "on the small island from a ladder in the helicopter. They raised her into the helicopter and took\n",
      "her to the school gym, where the Red Cross had set up an emergency shelter.  When the flood\n",
      "disappeared two days later, Nancy immediately went back to the \"island.\" Lizzie was gone. She was\n",
      "one of 19 cows that Nancy had lost in the flood. \"I owe my life to her,\" said Nancy with tears. \n",
      "\n",
      " \n",
      "There are a series of questions...\n",
      "What did Nancy try to do before she fell over?\n",
      "The following are true according to the passage except  _  .\n",
      "What did the local people do to help those in the flooded area according to the passage?\n",
      "Target: 0\n",
      "Resps: 3\n"
     ]
    }
   ],
   "source": [
    "get_race_examples(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "acbe281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: There is probably no field of human activity in which our values and lifestyles are shown more\n",
      "clearly and strongly than they are in the clothes that we choose to wear.The dress of an individual\n",
      "is a kind of \"sign language\" that communicates a set of information and is usually the basis on\n",
      "which immediate impressions are formed.Traditionally,a concern for clothes was considered to be an\n",
      "affair of females,while men took pride in the fact that they were completely lacking in clothes\n",
      "consciousness . This type of American culture is by degrees changing as man dress takes on greater\n",
      "variety and color.Even as early as 1955,a researcher in Michigan said that _ White collar workers in\n",
      "particular viewed dress as a symbol of ability,which could be used to impress or influence\n",
      "others,especially in the work situation.The white collar worker was described as extremely concerned\n",
      "about the impression his clothing made on his superiors .Although blue collar workers were less\n",
      "aware that they might be judged on the basis of their clothing,they recognized that any difference\n",
      "from the accepted pattern of dress would be made fun of by fellow workers. Since that time,of\n",
      "course,the patterns have changed:the typical office worker may now be wearing the blue shirt,and the\n",
      "laborer a white shirt;but the importance of dress has not become less.Other researchers in recent\n",
      "years have helped to prove its importance in the lives of individuals at various age levels and in\n",
      "different social and economic status groups . \n",
      "\n",
      " \n",
      "There are a series of questions...\n",
      "The passage tells us that   _  .\n",
      "Traditionally,people usually thought that   _  .\n",
      "Blue collar workers pay attention to their clothes because   _  .\n",
      "The passage mainly suggests that   _  .\n",
      "Target: 2\n",
      "Resps: 3\n"
     ]
    }
   ],
   "source": [
    "get_race_examples(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ece590-82e7-43d0-970e-b7110d912a4e",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data and Model for Prompt Tuning (20 points)\n",
    "Step-by-step directions:\n",
    "- Set the number of virtual tokens and training epochs (1-2 training epochs is fine) (2 points)\n",
    "- Make sure the tokenizer and model have a `pad` token set (2 points)\n",
    "- Load your training dataset and do a training/validation split (use a `seed` so you can reproduce the split in future homeworks and your final assignment), ensuring that your data are formatted in an instruction/response format (i.e., make sure each row denotes the instruction/question and response separately; for example, `\"Instruct: {instruction}\\n\\nResponse: {response}\"`). If you are using a `lm_eval` task for your training task, make sure your training/validation data are from the training split of the benchmark so that you are not directly training on test benchmark data. If you are not using a `lm_eval` task for your training task, make sure your training dataset does not contain examples from the test split you used above and that your evaluation dataset comes from a split of your training dataset so that you are not tuning performance on the test dataset directly. (10 points)\n",
    "- Tokenize your training and validation data using the map function (2 points)\n",
    "- Print 1 training and 1 validation data sample to make sure the mapping worked and the data are formatted properly (2 points)\n",
    "- Set up a `PromptTuningConfig` using your desired hyperparameters and print the number of trainable parameters (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6206f2fc-fe8b-4016-9824-9b2e8936585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_virtual_tokens = 10\n",
    "num_epochs = 2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cd6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"hellaswag\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6242c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:30000]\n",
    "val = data[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e91718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(train)\n",
    "val_dataset = Dataset.from_dict(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050028b",
   "metadata": {},
   "source": [
    "Unlike the instruction response used in other models HellaSwag uses a different format I have to write a function to format the instruction in this specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbc68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(example):\n",
    "    instruction = example['ctx']\n",
    "    correct_ending_index = int(example['label'])\n",
    "    response = example['endings'][correct_ending_index]\n",
    "\n",
    "    text = f\"Instruct: Complete the sentence: {instruction}\\n\\nResponse: {response}\"\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56ccea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9782549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524a17b32a9b406ab884a3b95456ac8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f740c055a89446d96c658bf1aa06aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_train_data = train_dataset.map(format_data, remove_columns=list(train_dataset.features))\n",
    "formatted_val_data = val_dataset.map(format_data, remove_columns=list(val_dataset.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e069626c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4809da54c3b4ca58cddbe54d57063b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9aadd4e494d49d0a30adf2d9a648db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = formatted_train_data.map(tokenize, batched=True, remove_columns=['text'])\n",
    "tokenized_val = formatted_val_data.map(tokenize, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab74f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [641,\n",
       "  1235,\n",
       "  25,\n",
       "  18608,\n",
       "  279,\n",
       "  11652,\n",
       "  25,\n",
       "  5005,\n",
       "  11,\n",
       "  279,\n",
       "  883,\n",
       "  13914,\n",
       "  916,\n",
       "  279,\n",
       "  11794,\n",
       "  18202,\n",
       "  279,\n",
       "  3241,\n",
       "  315,\n",
       "  264,\n",
       "  1803,\n",
       "  11,\n",
       "  323,\n",
       "  264,\n",
       "  5220,\n",
       "  12233,\n",
       "  12406,\n",
       "  15097,\n",
       "  42532,\n",
       "  13,\n",
       "  1221,\n",
       "  271,\n",
       "  2582,\n",
       "  25,\n",
       "  1154,\n",
       "  279,\n",
       "  883,\n",
       "  9539,\n",
       "  17592,\n",
       "  279,\n",
       "  11794,\n",
       "  389,\n",
       "  806,\n",
       "  1803,\n",
       "  13],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0714b170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [641,\n",
       "  1235,\n",
       "  25,\n",
       "  18608,\n",
       "  279,\n",
       "  11652,\n",
       "  25,\n",
       "  508,\n",
       "  2708,\n",
       "  60,\n",
       "  2585,\n",
       "  311,\n",
       "  990,\n",
       "  697,\n",
       "  2484,\n",
       "  8286,\n",
       "  508,\n",
       "  2102,\n",
       "  60,\n",
       "  95210,\n",
       "  279,\n",
       "  27505,\n",
       "  389,\n",
       "  279,\n",
       "  2115,\n",
       "  3108,\n",
       "  315,\n",
       "  279,\n",
       "  32177,\n",
       "  3250,\n",
       "  13,\n",
       "  508,\n",
       "  9520,\n",
       "  60,\n",
       "  576,\n",
       "  2484,\n",
       "  8286,\n",
       "  374,\n",
       "  264,\n",
       "  1293,\n",
       "  27505,\n",
       "  11,\n",
       "  5990,\n",
       "  3691,\n",
       "  476,\n",
       "  17545,\n",
       "  304,\n",
       "  1894,\n",
       "  13,\n",
       "  3197,\n",
       "  7726,\n",
       "  705,\n",
       "  476,\n",
       "  1495,\n",
       "  11,\n",
       "  419,\n",
       "  27505,\n",
       "  686,\n",
       "  5240,\n",
       "  264,\n",
       "  3100,\n",
       "  389,\n",
       "  2987,\n",
       "  279,\n",
       "  2115,\n",
       "  476,\n",
       "  1290,\n",
       "  3108,\n",
       "  315,\n",
       "  697,\n",
       "  1803,\n",
       "  311,\n",
       "  8217,\n",
       "  382,\n",
       "  2582,\n",
       "  25,\n",
       "  508,\n",
       "  1966,\n",
       "  24080,\n",
       "  60,\n",
       "  576,\n",
       "  2484,\n",
       "  8286,\n",
       "  686,\n",
       "  537,\n",
       "  1281,\n",
       "  264,\n",
       "  5112,\n",
       "  476,\n",
       "  3100,\n",
       "  279,\n",
       "  8286,\n",
       "  3100,\n",
       "  389,\n",
       "  697,\n",
       "  1803,\n",
       "  7241,\n",
       "  279,\n",
       "  1803,\n",
       "  374,\n",
       "  4303,\n",
       "  13,\n",
       "  508,\n",
       "  2102,\n",
       "  60,\n",
       "  5443,\n",
       "  279,\n",
       "  2484,\n",
       "  8286,\n",
       "  311,\n",
       "  13216,\n",
       "  264,\n",
       "  2484,\n",
       "  311,\n",
       "  279,\n",
       "  2115,\n",
       "  13],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b767099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ce9aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,480 || all params: 1,720,595,456 || trainable%: 0.0012\n"
     ]
    }
   ],
   "source": [
    "generation_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init = PromptTuningInit.RANDOM,\n",
    "    num_virtual_tokens=num_virtual_tokens,\n",
    "    tokenizer_name_or_path = 'Qwen/Qwen3-1.7B')\n",
    "\n",
    "model = get_peft_model(model, generation_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947c49f-7ac9-4aef-a26f-a3eec5a2e935",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Trainer and Train Model (20 points)\n",
    "Step-by-step directions:\n",
    "- As we did in class, create `TrainingArguments` that define an output directory (make sure the directory exists like we did in class); batch size; learning rate; number of training epochs; `logging_steps`, `eval_strategy`, and `eval_steps` for evaluating on the training and validation dataset; set the number of `save_steps`; and set `load_best_model_at_end` to `True` (5 points)\n",
    "- Create a trainer using your training arguments and defining the model, `train_dataset`, `eval_dataset`, and `data_collator` (5 points)\n",
    "- Run `trainer.train()` to train your model (5 points)\n",
    "- Save your model after training in a folder in your output_directory called best_model since the trainer will automatically load your best model at the end of training (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84e3d237-23b9-4170-b33e-ef95ab7d5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b1cc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_arguments(path, learning_rate= 0.001, epochs=num_epochs, eval_steps= 500):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, #specify path for trained model weights\n",
    "        auto_find_batch_size = True, #automatically find batch size\n",
    "        learning_rate = learning_rate,\n",
    "        num_train_epochs = epochs, \n",
    "        logging_steps = eval_steps, #this is how often we log training results\n",
    "        eval_strategy = \"steps\", #evaluate every 150 steps\n",
    "        eval_steps = eval_steps,\n",
    "        save_steps = eval_steps,\n",
    "        load_best_model_at_end = True,\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e593cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/scratch/ezq9qu\"\n",
    "output_directory = os.path.join(working_dir, \"prompt_tuned_trained_model\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e9e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = create_training_arguments(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76b09216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "def create_trainer(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                        mlm= False),\n",
    "    )\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93b48a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(model, training_args, tokenized_train, tokenized_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82d171b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpfoster317\u001b[0m (\u001b[33mpfoster317-university-of-virginia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sfs/gpfs/tardis/home/ezq9qu/lm-evaluation-harness/wandb/run-20251028_203906-winvz5p1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pfoster317-university-of-virginia/huggingface/runs/winvz5p1' target=\"_blank\">jumping-smoke-2</a></strong> to <a href='https://wandb.ai/pfoster317-university-of-virginia/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pfoster317-university-of-virginia/huggingface' target=\"_blank\">https://wandb.ai/pfoster317-university-of-virginia/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pfoster317-university-of-virginia/huggingface/runs/winvz5p1' target=\"_blank\">https://wandb.ai/pfoster317-university-of-virginia/huggingface/runs/winvz5p1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 43:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.731100</td>\n",
       "      <td>2.228446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.297400</td>\n",
       "      <td>2.069973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.205200</td>\n",
       "      <td>2.040892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.177800</td>\n",
       "      <td>2.016444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.162300</td>\n",
       "      <td>2.003463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.149500</td>\n",
       "      <td>1.992006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.139800</td>\n",
       "      <td>1.987259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>1.978879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.126600</td>\n",
       "      <td>1.978730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.978866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.131300</td>\n",
       "      <td>1.971821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.116200</td>\n",
       "      <td>1.973276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.102100</td>\n",
       "      <td>1.970197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.101900</td>\n",
       "      <td>1.968646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.103500</td>\n",
       "      <td>1.968127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=2.1856515787760418, metrics={'train_runtime': 2611.9547, 'train_samples_per_second': 22.971, 'train_steps_per_second': 2.871, 'total_flos': 6.482465380388045e+16, 'train_loss': 2.1856515787760418, 'epoch': 2.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fbe40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(f\"{output_directory}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593bc30-ab22-4618-b274-02db52a79d99",
   "metadata": {},
   "source": [
    "## Step 4: Assess Post-training Performance (20 points)\n",
    "Step-by-step directions:\n",
    "- Assess post training performance: Set up an `lm_eval` `task_manager` and implement your training task's test split and general language benchmark; make sure to log the samples and you can set a limit (n=50) if you'd like. If you are implementing a training task not in `lm_eval`, remember to use your held-out test data, logging responses and using a limit if you'd like. (10 points)\n",
    "- Print the results and 2 model responses for each task to get a sense of what the model outputs look like after training (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29bd05bd-1f1e-4b4c-be19-51349f8057d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 50/50 [00:00<00:00, 4612.57it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 3536.63it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 400/400 [00:17<00:00, 23.20it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = \"hf\", #Specify huggingface model\n",
    "    model_args = {\"pretrained\": model, \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer}, #Define model arguments\n",
    "    tasks = [\"hellaswag\", \"race\"], \n",
    "    log_samples = True, \n",
    "    batch_size = \"1\",\n",
    "    limit = 50,\n",
    "    random_seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dff82b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.44,\n",
       "  'acc_stderr,none': 0.07091242083423345,\n",
       "  'acc_norm,none': 0.5,\n",
       "  'acc_norm_stderr,none': 0.07142857142857142},\n",
       " 'race': {'alias': 'race',\n",
       "  'acc,none': 0.34,\n",
       "  'acc_stderr,none': 0.0676726816132972}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19ae5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man: \n",
      "\n",
      "are then shown paddling down a river in a boat while a woman talks.\n",
      "are driving the canoe, they go down the river flowing side to side.\n",
      "sit in a canoe while the man paddles.\n",
      "walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.\n",
      "\n",
      "\n",
      "Target: 2\n",
      "Response: 2\n"
     ]
    }
   ],
   "source": [
    "get_hella_examples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ada0cf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: High jump: A boy is running down a track. The boy: \n",
      "\n",
      "runs into a car.\n",
      "gets in a mat.\n",
      "lifts his body above the height of a pole.\n",
      "stands on his hands and springs.\n",
      "\n",
      "\n",
      "Target: 2\n",
      "Response: 1\n"
     ]
    }
   ],
   "source": [
    "get_hella_examples(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9c3d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Little Tommy was doing very badly in math. His parents had tried everything--tutors, cards, special\n",
      "learning centers--in short, everything they could think of. Finally they took Tommy to a catholic\n",
      "school. After the first day, little Tommy came home with a very serious look on his face. He didn't\n",
      "kiss his mother hello. Instead, he went straight to his room and started studying. Books and papers\n",
      "were spread out all over the room and little Tommy was hard at work. His mother was surprised. She\n",
      "called him down to dinner and as soon as he finished eating, he went back to his room, without a\n",
      "word. In no time he was back hitting the books as hard as before. This went on for some time, day\n",
      "after day while the mother tried to understand what was happening. Finally, little Tommy brought\n",
      "home his report card. He quietly put it on the table and went up to his room and hit the books. His\n",
      "mom looked at it and to her surprise, little Tommy got an A in math. She could no longer hold her\n",
      "curiosity. She went to his room and asked, \"Son, what was it? Was it the nuns ? \" Little Tommy\n",
      "looked at her and shook his head, \"No. \" \"Well then,\" she asked again. \"WHAT was it? \" Little Tommy\n",
      "looked at her and said, \"Well, on the first day of school, when I saw that man nailed to the plus\n",
      "sign , I knew they weren't joking. \" \n",
      "\n",
      " \n",
      "There are a series of questions...\n",
      "Tommy's mother felt surprised that his son  _  .\n",
      "The last sentence in the passage shows that  _  .\n",
      "From the passage, we can infer that  _  .\n",
      "Target: 1\n",
      "Resps: 0\n"
     ]
    }
   ],
   "source": [
    "get_race_examples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a50e7dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Give it five minutes I used to be a hothead. Whenever anyone said anything, I'd think of a way to\n",
      "disagree. I'd push back hard if something didn't fit my world-view. It's like I had to be first with\n",
      "an opinion -- as if being first meant something. But what it really meant was that I wasn't thinking\n",
      "hard enough about the problem. The faster you react, the less you think. Not always, but often. This\n",
      "came to a head back in 2007. I was speaking at the Business Innovation Factory conference in\n",
      "Providence, RI. So was Richard Saul Wurman. After my talk Richard came up to introduce himself and\n",
      "compliment my talk. That was very generous of him. He certainly didn't have to do that. And what did\n",
      "I do? I pushed back at him about the talk he gave. While he was making his points on stage, I was\n",
      "taking an inventory of the things I didn't agree with. And when presented with an opportunity to\n",
      "speak with him, I quickly pushed back at some of his ideas. I must have seemed like such an asshole.\n",
      "His response changed my life. It was a simple thing. He said \"Man, give it five minutes.\" I asked\n",
      "him what he meant by that? He said, it's fine to disagree, it's fine to push back, it's great to\n",
      "have strong opinions and beliefs, but give my ideas some time to set in before you're sure you want\n",
      "to argue against them. \"Five minutes\" represented \"think\", not react. He was totally right. I came\n",
      "into the discussion looking to prove something, not learn something. This was a big moment for me.\n",
      "Richard has spent his career thinking about these problems. He's given it 30 years. And I gave it\n",
      "just a few minutes. Now, certainly he can be wrong and I could be right, but it's better to think\n",
      "deeply about something first before being so certain you're right. There's also a difference between\n",
      "asking questions and pushing back. Pushing back means you already think you know. Asking questions\n",
      "means you want to know. Ask more questions. Learning to think first rather than react quick is a\n",
      "life-long pursuit. It's tough. I still get hot sometimes when I shouldn't. But I'm really enjoying\n",
      "all the benefits of getting better. If you aren't sure why this is important, think about this quote\n",
      "from Jonathan Ive regarding Steve Jobs' reverence(respect) for ideas: And just as Steve loved ideas,\n",
      "and loved making stuff, he treated the process of creativity with a rare and a wonderful reverence.\n",
      "You see, I think he better than anyone understood that while ideas ultimately can be so powerful,\n",
      "they begin as fragile, barely formed thoughts, so easily missed, so easily compromised, so easily\n",
      "just squished. That's deep. Ideas are fragile. They often start powerless. They're barely there, so\n",
      "easy to ignore or skip or miss. There are two things in this world that take no skill: 1. Spending\n",
      "other people's money and 2. Dismissing an idea. Dismissing an idea is so easy because it doesn't\n",
      "involve any work. You can scoff at it. You can ignore it. You can puff some smoke at it. That's\n",
      "easy. The hard thing to do is protect it, think about it, let it marinate, explore it, riff on it,\n",
      "and try it. The right idea could start out life as the wrong idea. So next time you hear something,\n",
      "or someone, talk about an idea, pitch an idea, or suggest an idea, give it five minutes. Think about\n",
      "it a little bit before pushing back, before saying it's too hard or it's too much work. Those things\n",
      "may be true, but there may be another truth in there too: It may be worth it. \n",
      "\n",
      " \n",
      "There are a series of questions...\n",
      "What did the author do while Richard was talking in the business conference?\n",
      "Which of the following is the reason for quoting Jonathan Ive?\n",
      "What is the core argument that the author put forward?\n",
      "Target: 2\n",
      "Resps: 3\n"
     ]
    }
   ],
   "source": [
    "get_race_examples(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa5c4e-447a-4a99-abe3-d44fead029eb",
   "metadata": {},
   "source": [
    "## Step 5: Interpretation (20 points)\n",
    "In one-two paragraphs, interpret the results from prompt tuning. Discuss whether you think prompt tuning worked for your task, citing quantitative and qualitative evidence from Steps 1 and 4 (10 points). Also discuss why you think prompt tuning performed the way it did for your task (i.e., why did it succeed or fail?), citing any resources you used to formulate your argument (10 points). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fef17",
   "metadata": {},
   "source": [
    "The model has become slightly better at the specific task, hellaswag, after prompt tuning. We can see evidence of this from the fact that the model performed better on it metrics. The model improved from being correct 40 percent of the time to 44 percent. This gain in performance did come at a cost. We can see that the other metric, race, went down as a result. Race went from a peformance of 36 percent down to 34%. I think that ti worked specifically in this case because the dataset was particularly large and well formatted. Since I used a dataset that was available from the lm-eval harness it allowed for a dataset with almost 40,000 entries, this gave the model a lot of training data to learn from. Also, the data was of good quality, a lot of work has gone into maintaining good quality prompts in the hellaswag dataset. These two factors allowed for an improvement in performance across the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed955",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
