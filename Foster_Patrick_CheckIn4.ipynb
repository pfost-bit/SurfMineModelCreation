{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a72899-a701-4559-90bb-791f2a990807",
   "metadata": {},
   "source": [
    "# Final Project Check-in 4\n",
    "The goal of this check in is to fully implement your desired training approach for your task and evaluate pre and post training performance on your full evaluation benchmark datasets. For RAG tasks, this will involve implementing your full RAG pipeline and fully evaluating your pipeline on your RAG-specific benchmarks. This notebook will guide you through the necessary steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddd1ff-eaa8-4c4c-b126-0c35b489e394",
   "metadata": {},
   "source": [
    "## Step 1: Choose Your Training/RAG Approach (15 points)\n",
    "In a markdown cell below, state which training/RAG method you plan to implement for your task. In one-two paragraphs, summarize why you chose that training/RAG approach, citing both things you have learned in class and your empirical results from implementing different training/RAG approaches for your task in the past two homeworks or your own experiments (in the case of RAG). Also describe any drawbacks you anticipate from the training/RAG approach you chose and why the advantages you anticipate outweigh those drawbacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cec873",
   "metadata": {},
   "source": [
    "I plan to implement the PEFT method of LoRA for my training task. Specifically I plan to:\n",
    "\n",
    "1. Load the base model: `Qwen/Qwen3-4B-Instruct-2507`\n",
    "2. Load the dataset that I created previously (live forecast data/human forecast/instruction pairs)\n",
    "3. Apply a Chat template\n",
    "4. See the benchmark of the model using BERT/ROUGE-L/BLEU\n",
    "5. Train the Model\n",
    "6. Save and see new performance Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c280ba",
   "metadata": {},
   "source": [
    "In the past two homeworks we have implemented trainers that helped to fine-tune specific outcomes needed for base model. My dataset and task do not have a base dataset created in order to look at quantative metrics to see how the Model learns. The best metric that I could find was BERT, which calculates the simularity between two tokenized inputs. I can compare the inputs from the validation split and see if the BERT F1 score increases from pre-training to post training versions of the model. \n",
    "\n",
    "I have chosen to implement LoRA as I do not need to change the behavior of the model to the point where full fine tuning becomes necessary. So by implementing the LoRA PEFT method I can approach full fine tuning while hopefully maintaining some other functionality and decreasing the time to train. The biggest downside to this approach is the fact that it will take a long time to train, as I plan on letting it run for at least 5 full epochs on the full training dataset. This means I will need to have the code running while I am not actively monitoring it's progress. The advantage is obvious, I get a model that will be fine-tuned for my specific task that should perform better than the simple In Context Learnings from previous check-ins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd97bb4",
   "metadata": {},
   "source": [
    "The performance metrics will be more difficult to implement so, I plan to use the `lm_eval` library to generate the results after tuning and then directly comparing teh generated text with the correct text to find the BERT/ROUGE-L/BLEU scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e7e79-21d7-4856-b484-4d0703f4a637",
   "metadata": {},
   "source": [
    "## Step 2: Benchmark Your Model (20 points)\n",
    "As you have done in your past two homeworks, use the `lm_eval` package or custom code to evaluate your model's pre-training/pre-RAG performance on your 3 benchmark tasks and testing split from your training/RAG data (use the same testing split that you used in the homeworks for weeks 9 and 10), this time on the full datasets without setting a limit (5 points). For RAG tasks, this will involve implementing the benchmark prompts without any retrieval of relevant documents (the model should perform very poorly, showing a need for RAG). Make sure to log samples as we have done in the past and print the results in a code cell below (5 points). For open-ended generation tasks, you can use a slurm script to benchmark your model since they may take a long time to run. If you go this route, make sure to save your results and model responses as separate json files and load and display them in a code cell below. \n",
    "\n",
    "With either approach, make sure to print 2 model responses (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45566870",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    " * I saved the training dataset as a `.csv` previously and can access it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6052c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"HF_HOME\"] = \"/scratch/ezq9qu/models/cache\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from tqdm import tqdm\n",
    "from lm_eval import evaluator\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25daa0f0-64e9-42a2-9385-b279d4bacb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_993911/2322644220.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data.rename(columns={\"nws_forecast\":\"prompt_text\", \"human_forecast\":\"Response\"},inplace=True)\n",
      "/tmp/ipykernel_993911/2322644220.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data[\"Instruct\"] = \"Q: \" + instruction_text + training_data[\"prompt_text\"]+\" Let's think step by step\\nA: \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_data = pd.read_csv(\"training_data(1).csv\")\n",
    "\n",
    "training_data = all_data[[\"nws_forecast\",\"human_forecast\"]]\n",
    "training_data.rename(columns={\"nws_forecast\":\"prompt_text\", \"human_forecast\":\"Response\"},inplace=True)\n",
    "\n",
    "\n",
    "instruction_text = \"\"\"\n",
    "Output a human-readable surf-forecast similar to that of a veteran surf-obsever. The response should take into account the winds, sea-state, and wave period. The final output should be a few short sentences, with some surfing lingo and flair. The data is as follows:\n",
    "\"\"\"\n",
    "\n",
    "training_data[\"Instruct\"] = \"Q: \" + instruction_text + training_data[\"prompt_text\"]+\" Let's think step by step\\nA: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7360be",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9f2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_temp = train_test_split(final_data, train_size=0.8, random_state=126)\n",
    "df_val, df_test = train_test_split(df_temp, train_size=0.5, random_state=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81e398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\":Dataset.from_pandas(df_train),\n",
    "    \"validation\":Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea5eeb",
   "metadata": {},
   "source": [
    "There is now a 80/10/10 train/test/val split on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ea9328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f348b09cf915400781bad5a69a546f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Interactive login (recommended)\n",
    "login() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d262df",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76a4549a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6752fe6976614425a6a7d131f53a1d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf385922ce344128a0b06ad25ee3bc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2538bcae30094a3eafba11c168b37984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a0c88aa67b4bcd82be11f29b493f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961f89a7909848e8b75dacfd0a343e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e333e4a29044298cfe9e07a05b9305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066183daf0404e85a7727fc1f282a00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ac836d86364170940d3ea7ce4b5cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63a7b6a85ac4a4ca1132337263ae6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9391adcc87eb4b74bbe9bf75bf051924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bf565d04c34d06a47a403c203ca6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8542aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_virtual_tokens = 10\n",
    "num_epochs = 5\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c44e1a",
   "metadata": {},
   "source": [
    "Now I we have to get the Example Prompt and the Response and compare them to generate the baseline metrrics, we cannot use a simple lm_eval test as they don't have a built in task for my specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "627aba90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7323a69ab3422a8d91e9d257668069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5afda438ff14ea68915bd791f070ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/89 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = dataset[\"train\"].map(lambda samples: tokenizer(samples['Instruct']), batched = True)\n",
    "val = dataset[\"validation\"].map(lambda samples: tokenizer(samples['Instruct']), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bece0b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_text': 'Wind: SW winds 5 kt, Seas: 2 ft, Wave Detail: E 1 ft at 4 seconds and S 1 ft at 4 seconds.',\n",
       " 'Response': 'Hey everyone, there’s no waves out back behind the shop right now. It looks like a lake out there. Buoys are reading 2.3 ft @ 3.8 seconds, with winds blowing 14 mph SSW. Clouds are still lingering over the beach, but besides that it’s a fairly nice day out. The waves that are breaking are […]',\n",
       " 'Instruct': \"Q: \\nOutput a human-readable surf-forecast similar to that of a veteran surf-obsever. The response should take into account the winds, sea-state, and wave period. The final output should be a few short sentences, with some surfing lingo and flair. The data is as follows:\\nWind: SW winds 5 kt, Seas: 2 ft, Wave Detail: E 1 ft at 4 seconds and S 1 ft at 4 seconds. Let's think step by step\\nA: \",\n",
       " '__index_level_0__': 731,\n",
       " 'input_ids': [2,\n",
       "  235368,\n",
       "  235292,\n",
       "  235248,\n",
       "  108,\n",
       "  6140,\n",
       "  476,\n",
       "  3515,\n",
       "  235290,\n",
       "  78823,\n",
       "  23238,\n",
       "  235290,\n",
       "  82773,\n",
       "  3968,\n",
       "  577,\n",
       "  674,\n",
       "  576,\n",
       "  476,\n",
       "  33244,\n",
       "  23238,\n",
       "  235290,\n",
       "  1023,\n",
       "  27169,\n",
       "  235265,\n",
       "  714,\n",
       "  3590,\n",
       "  1412,\n",
       "  1987,\n",
       "  1280,\n",
       "  3185,\n",
       "  573,\n",
       "  27626,\n",
       "  235269,\n",
       "  4176,\n",
       "  235290,\n",
       "  2626,\n",
       "  235269,\n",
       "  578,\n",
       "  9591,\n",
       "  4037,\n",
       "  235265,\n",
       "  714,\n",
       "  2048,\n",
       "  5033,\n",
       "  1412,\n",
       "  614,\n",
       "  476,\n",
       "  2619,\n",
       "  3309,\n",
       "  26099,\n",
       "  235269,\n",
       "  675,\n",
       "  1009,\n",
       "  71106,\n",
       "  533,\n",
       "  13922,\n",
       "  578,\n",
       "  47936,\n",
       "  235265,\n",
       "  714,\n",
       "  1423,\n",
       "  603,\n",
       "  685,\n",
       "  6397,\n",
       "  235292,\n",
       "  108,\n",
       "  32572,\n",
       "  235292,\n",
       "  13179,\n",
       "  27626,\n",
       "  235248,\n",
       "  235308,\n",
       "  80132,\n",
       "  235269,\n",
       "  71032,\n",
       "  235292,\n",
       "  235248,\n",
       "  235284,\n",
       "  9801,\n",
       "  235269,\n",
       "  30353,\n",
       "  32889,\n",
       "  235292,\n",
       "  637,\n",
       "  235248,\n",
       "  235274,\n",
       "  9801,\n",
       "  696,\n",
       "  235248,\n",
       "  235310,\n",
       "  10102,\n",
       "  578,\n",
       "  570,\n",
       "  235248,\n",
       "  235274,\n",
       "  9801,\n",
       "  696,\n",
       "  235248,\n",
       "  235310,\n",
       "  10102,\n",
       "  235265,\n",
       "  4371,\n",
       "  235303,\n",
       "  235256,\n",
       "  1742,\n",
       "  4065,\n",
       "  731,\n",
       "  4065,\n",
       "  108,\n",
       "  235280,\n",
       "  235292,\n",
       "  235248],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd22a262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    "    do_sample = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb610652",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = text_gen(\n",
    "    val[\"Instruct\"],\n",
    "    batch_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5836afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for output in outputs:\n",
    "    full_text = output[0]['generated_text']\n",
    "    text = full_text.rsplit(\"A:\",1)[-1]\n",
    "    predictions.append(text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f03c7",
   "metadata": {},
   "source": [
    "Now we can look at the predictions and the expected results prior to fine tuning, the difference is going to be rather large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79f57cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Start with the wind: NE 5 kt is light and offshore, which is great — it helps keep the waves clean and prevents wind chop.  \\n2. Look at the sea state: 2 ft is a modest swell, not massive, but manageable.  \\n3. Analyze the wave detail: SE 2 ft at 8 seconds — that’s a long-period swell, which means it’ll ride smooth and carve well, perfect for experienced surfers. The E 1 ft at 5 seconds is a shorter, more choppy wave, likely from local wind or swell, not ideal for long rides.  \\n4. Combine the elements: The long-period wave from the SE gives you a solid, predictable ride with good hold and shape. The offshore wind keeps the surface clean, and the wave period is ideal for carving.  \\n5. Final output:  \\n\"Good clean conditions out there — offshore NE wind keeps the water smooth, and the SE swell at 8 seconds delivers long, powerful, and rideable waves. Perfect for carving and catching the cut. Watch out for the shorter, choppy E swell though — not ideal for long rides. Go out with a clean board and a steady hand.\"  \\n\\nThis response uses surfing lingo, is'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3834fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = val[\"Response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "246fa216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey guys! There is a little longboard wave out back, but it is pretty calm. The ocean surface is clean, knee high, with barely any wind. The tide is going out, with low tide at 6:30pm. If you have time this evening, try to paddle out! Keep an eye on the cam and check back […]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f040f6",
   "metadata": {},
   "source": [
    "We can now calculate some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f7f3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load('rouge')\n",
    "bert_metric = evaluate.load('bertscore')\n",
    "bleu_metric = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4295e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = rouge_metric.compute(predictions=predictions,references=references)\n",
    "bert_scores = bert_metric.compute(predictions=predictions,references=references, lang= 'en')\n",
    "bleu_score = bleu_metric.compute(predictions=predictions,references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64ea17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(rouge_score, bert_score, bleu_metric):\n",
    "\n",
    "    print(predictions[0])\n",
    "    print(\"\\n\")\n",
    "    print(references[0])\n",
    "\n",
    "\n",
    "    print(\"\\n--- ROUGE Scores ---\")\n",
    "    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "    print(f\"  **ROUGE-L: {rouge_scores['rougeL']:.4f}**\")\n",
    "\n",
    "    print(\"\\n--- BLEU Score ---\")\n",
    "    print(f\"  **BLEU: {bleu_score['bleu']:.4f}**\")\n",
    "\n",
    "    print(\"\\n--- BERTScore ---\")\n",
    "    avg_f1 = np.mean(bert_scores['f1'])\n",
    "    print(f\"  **Average F1: {avg_f1:.4f}**\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a910a72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step 1: Analyze the wind:**\n",
      "\n",
      "* NE winds at 5 knots are a bit of a mixed bag. They'll be pushing the waves around, but not too much to make things gnarly.\n",
      "\n",
      "**Step 2: Assess the sea state:**\n",
      "\n",
      "* 2 feet of swell is a solid base for some fun. It's not huge, but it'll provide some decent size and shape.\n",
      "\n",
      "**Step 3: Examine the wave detail:**\n",
      "\n",
      "* The wave set-up is a bit of a puzzle. We've got a solid 2-foot set at 8 seconds, which is a good indicator of some fun, clean waves. But, there's also a smaller set at 5 seconds, which could mean some choppy sections.\n",
      "\n",
      "**Step 4: Combine the information:**\n",
      "\n",
      "* Overall, it looks like a decent day for a solid session. The 2-foot waves with a good period will be fun, but keep an eye out for the choppy sections. \n",
      "\n",
      "**Final Output:**\n",
      "\n",
      "\"Looks like a solid day for some beach breaks, with a mix of clean 2-footers and some choppy sections.  Keep an eye out for the 5\n",
      "\n",
      "\n",
      "Hey guys! There is a little longboard wave out back, but it is pretty calm. The ocean surface is clean, knee high, with barely any wind. The tide is going out, with low tide at 6:30pm. If you have time this evening, try to paddle out! Keep an eye on the cam and check back […]\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "  ROUGE-1: 0.2075\n",
      "  ROUGE-2: 0.0306\n",
      "  **ROUGE-L: 0.1227**\n",
      "\n",
      "--- BLEU Score ---\n",
      "  **BLEU: 0.0059**\n",
      "\n",
      "--- BERTScore ---\n",
      "  **Average F1: 0.8201**\n"
     ]
    }
   ],
   "source": [
    "show_results(rouge_score=rouge_scores,bert_score=bert_scores,bleu_metric=bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0c970",
   "metadata": {},
   "source": [
    "This shows that the model is currently a repeater, it just repeats what it has been pre-trained on without actually using any actual surfer lingo. This is evidenced byt the low ROUGE and BLEU scores. However the High BERT score shows that the model does pretty well semantically, the initial picture is pretty good. The goal will be to make the ROUGE and BLEU scores go up (bring in some more surfer lingo), while maintaining the high bert score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a17ed-0d9e-4bc9-977f-a99eed6c2eba",
   "metadata": {},
   "source": [
    "## Step 3: Train Your Model or Implement Your RAG Pipeline (25 points)\n",
    "For finetuning tasks:\n",
    "As you have done in the past two homeworks, prepare your data for training and train your model using the HuggingFace `trainer` (making sure to do a train/eval split of your training data (use the same train/eval split you used in your homeworks for weeks 9 and 10) and log metrics during training), loading and saving the best model at the end (10 points). Try at least three different hyperparameter combinations to find values that work well for your task (10 points). You can show results for each hyperparameter combination, or you can just show results for the best combination. In a markdown cell below your results in one paragraph, describe the values you tried, why you chose them, and which ones worked best (5 points).\n",
    "\n",
    "For RAG tasks: \n",
    "Pull in your custom dataset and set up your RAG pipeline, including tokenizing your dataset, embedding it, storing the embeddings, and setting up the retrieval module based on a similarity metric (10 points). Try at least 3 different combinations of embedding and retrieval metrics to find what pipeline works best for your task, comparing performance for the different combinations based on performance on your manually-constructed test prompts for your task (10 points). In a markdown cell below your results in one paragraph, describe the combinations you tried, why you chose them, and which ones worked best (5 points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3fa1095-63e0-45cf-a830-04cf397757fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 23,592,960 || all params: 4,046,061,056 || trainable%: 0.5831\n"
     ]
    }
   ],
   "source": [
    "LORA_R = 64 \n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = .05\n",
    "lora_config = LoraConfig(\n",
    "    r = LORA_R, #the lower dimension of the low-rank matrices\n",
    "    lora_alpha = LORA_ALPHA, #scaling factor for the low-rank update\n",
    "    lora_dropout = LORA_DROPOUT, #dropout factor to prevent overfitting\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\", #set language modeling as task type\n",
    "    target_modules = [\"q_proj\", \"v_proj\"], #add LoRA modules to every query and value matrix in the attention layers\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dec0110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "def create_training_arguments(path, learning_rate = 0.000001, epochs = 5, eval_steps = 100):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, #specify path for trained model weights\n",
    "        auto_find_batch_size = True, #automatically find batch size\n",
    "        learning_rate = learning_rate,\n",
    "        num_train_epochs = epochs, \n",
    "        logging_steps = eval_steps, #this is how often we log training results\n",
    "        eval_strategy = \"steps\", #evaluate every 150 steps\n",
    "        eval_steps = eval_steps,\n",
    "        save_steps = eval_steps,\n",
    "        load_best_model_at_end = True,\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b83be4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/scratch/ezq9qu\"\n",
    "output_directory = os.path.join(working_dir, \"trained_lora_model_surfer_one\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1dc94372",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = create_training_arguments(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67665543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "def create_trainer(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                        mlm= False),\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d479b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(model, training_args, train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e0a7bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 03:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.276540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.274848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.274187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.273797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=0.28052342096964517, metrics={'train_runtime': 204.1422, 'train_samples_per_second': 17.537, 'train_steps_per_second': 2.204, 'total_flos': 1.0844837372276736e+16, 'train_loss': 0.28052342096964517, 'epoch': 5.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e2720eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(f\"{output_directory}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987aa8d",
   "metadata": {},
   "source": [
    "This is how I implemented one batch of training. I was able to tune the hyperparameters using a python script and slurm. Google Gemini helped in the creation of the python file for sweeping through the hyperparameters.\n",
    "\n",
    "The python file essentialy looped through a bunch of different hyperparameters and trained different models every time this could be done natively inside the `wandb` libray. It allowed me to save the best performing model to a file, it is saved in my `\\scratch` folder. The best hyperparameters were:\n",
    "\n",
    "* learning_rate: 0.00012929155174079129 \n",
    "* lora_r: 64\n",
    "* lora_alpha: 32\n",
    "* lora_dropout: 0.1\n",
    "* num_train_epochs: 5\n",
    "* per_device_train_batch_size: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc586a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd567b5e6a34c3e8dbc2362019ef0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained('/scratch/ezq9qu/wandb-sweeps/stellar-sweep-4/best_model/', device_map=\"auto\",dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e41d09-cedd-4787-87df-3af3355a500d",
   "metadata": {},
   "source": [
    "## Step 4: Assess Post-training Benchmark Performance (20 points)\n",
    "Repeat Step 2 using your trained model or RAG pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e6edab7-b011-4868-86a9-81df3927d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    "    do_sample = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf758383",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = text_gen(\n",
    "    val[\"Instruct\"],\n",
    "    batch_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb123743",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for output in outputs:\n",
    "    full_text = output[0]['generated_text']\n",
    "    text = full_text.rsplit(\"A:\",1)[-1]\n",
    "    predictions.append(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3667f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = val[\"Response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec16a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load('rouge')\n",
    "bert_metric = evaluate.load('bertscore')\n",
    "bleu_metric = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d6216dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = rouge_metric.compute(predictions=predictions,references=references)\n",
    "bert_scores = bert_metric.compute(predictions=predictions,references=references, lang= 'en')\n",
    "bleu_score = bleu_metric.compute(predictions=predictions,references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "533c59d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ft SE waves at 8 seconds, with a bit of E 1 ft at 5 seconds. Light winds and seas, so it could be a bit of surf. 2 ft SE at 8 seconds, with a bit of E 1 ft at 5 seconds. Light winds and seas, so it could be a bit of surf. 2 ft SE at 8 seconds, with a bit of E 1 ft at 5 seconds. Light winds and seas, so it could be a bit\n",
      "\n",
      "\n",
      "Hey guys! There is a little longboard wave out back, but it is pretty calm. The ocean surface is clean, knee high, with barely any wind. The tide is going out, with low tide at 6:30pm. If you have time this evening, try to paddle out! Keep an eye on the cam and check back […]\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "  ROUGE-1: 0.1477\n",
      "  ROUGE-2: 0.0162\n",
      "  **ROUGE-L: 0.1069**\n",
      "\n",
      "--- BLEU Score ---\n",
      "  **BLEU: 0.0046**\n",
      "\n",
      "--- BERTScore ---\n",
      "  **Average F1: 0.8132**\n"
     ]
    }
   ],
   "source": [
    "show_results(rouge_score=rouge_scores,bert_score=bert_scores,bleu_metric=bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4354cb-edfb-4bf8-9638-456ad594a3f8",
   "metadata": {},
   "source": [
    "## Step 5: Interpretation of Results (20 points)\n",
    "\n",
    "For finetuning tasks:\n",
    "In one-two paragraphs, summarize the results from training your model, noting how the outputs improved post training, how performance on the benchmarks and testing split changed post training, and whether and how the improvements and drawbacks from training you noticed empirically matched those you anticipated in Step 1 above. \n",
    "\n",
    "For RAG tasks:\n",
    "In one-two paragraphs, summarize the results from implementing RAG with your model, noting how the outputs improved after implementing the RAG pipeline, how performance on the benchmarks and testing split changed after implementing RAG, and whether and how the improvements and drawbacks from RAG you noticed empirically matched those you anticipated in Step 1 above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac08e67",
   "metadata": {},
   "source": [
    "It did not get the desired results from my implemented training. THe Rouge and BLEU scores did not increase and the BERT score went down slighty. This does not seem to get when I need, so in order to move forward get better results I have to add someway to incentivise the correct output formatting. This might be full fine tuning combined with some fine-tuning steps like in-context learning first, then followed by fine tuning. By implementing the training this way I hope to get the model to learn the correct output style first, then we can add more specifics with fine-tuning. I should be able to accomplish this with some few shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f4442",
   "metadata": {},
   "source": [
    "## Additional Code \n",
    "\n",
    "Here is the code I wrote for the python training and slurm job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ae336",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (927569835.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m```{bash}\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -A ds5002 ##Define allocation\n",
    "#SBATCH --partition=gpu ##Define GPU partition\n",
    "#SBATCH --gres=gpu:2 ##Specify desired number of GPUs\n",
    "#SBATCH --constraint=a6000 ##Optional: specify type of GPU\n",
    "#SBATCH --ntasks=1 ##Specify number of tasks \n",
    "\n",
    "#SBATCH --cpus-per-task=2 ##Specify number of CPUs per task\n",
    "#SBATCH --mem=10G ##Specify amount of CPU storage needed\n",
    "#SBATCH -t 1-00:00:00 ##Specify time constraint in Days-Hours:Minutes:Seconds format\n",
    "#SBATCH -J wandb-sweep ##Name the job\n",
    "#SBATCH -o wandb-sweep-%A.out ##Provide a name for the .out file- this is where any output printed to console will be stored after the job finishes\n",
    "#SBATCH -e wandb-sweep-%A.err ##Provide a name for the .err file- this is where any error messages and output will be printed during the job\n",
    "#SBATCH --mail-user=ezq9qu@virginia.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module purge ##Purge any existing modules on the compute resources\n",
    "module load miniforge ##Load miniforge for python\n",
    "source activate /scratch/ezq9qu/llm_course ##Load your virtual environment\n",
    "python sweep.py ##Run your .py file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ffc78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb  \n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/ezq9qu/models/cache\" \n",
    "os.environ[\"WANDB_PROJECT\"] = \"surf-forecast-lora-sweep\"\n",
    "WORKING_DIR = \"/scratch/ezq9qu\"\n",
    "\n",
    "\n",
    "def load_surf_dataset():\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the surf forecast dataset.\n",
    "    \"\"\"\n",
    "    all_data = pd.read_csv(\"training_data(1).csv\")\n",
    "    training_data = all_data[[\"nws_forecast\", \"human_forecast\"]]\n",
    "\n",
    "    training_data = training_data.rename(columns={\"nws_forecast\": \"prompt_text\", \"human_forecast\": \"Response\"})\n",
    "\n",
    "    instruction_text = \"\"\"\n",
    "Output a human-readable surf-forecast similar to that of a veteran surf-obsever. The response should take into account the winds, sea-state, and wave period. The final output should be a few short sentences, with some surfing lingo and flair. The data is as follows:\n",
    "\"\"\"\n",
    "    # Create the \"Instruct\" column\n",
    "    training_data = training_data.assign(\n",
    "        Instruct=\"Q: \" + instruction_text + training_data[\"prompt_text\"] + \" Let's think step by step\\nA: \"\n",
    "    )\n",
    "    \n",
    "    final_data = training_data\n",
    "    \n",
    "    df_train, df_temp = train_test_split(final_data, train_size=0.8, random_state=126)\n",
    "    df_val, df_test = train_test_split(df_temp, train_size=0.5, random_state=126)\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(df_train),\n",
    "        \"validation\": Dataset.from_pandas(df_val),\n",
    "        \"test\": Dataset.from_pandas(df_test)\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "# --- Tokenization Function ---\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Applies tokenization to the dataset.\n",
    "    \"\"\"\n",
    "    def tokenize_function(samples):\n",
    "        # Tokenize the 'Instruct' field\n",
    "        return tokenizer(samples['Instruct'], truncation=True, padding=False)\n",
    "\n",
    "    train_dataset = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "    val_dataset = dataset[\"validation\"].map(tokenize_function, batched=True)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    This function is called by the wandb agent for each trial.\n",
    "    \"\"\"\n",
    "    # 1. Initialize wandb run\n",
    "    # This will fetch the hyperparams for this specific run\n",
    "    run = wandb.init() \n",
    "    config = wandb.config\n",
    "\n",
    "    # --- Load Tokenizer and Datasets ---\n",
    "    model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "    # Set pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    dataset = load_surf_dataset()\n",
    "    if dataset is None:\n",
    "        return # Stop if data loading failed\n",
    "\n",
    "    train_dataset, val_dataset = tokenize_dataset(dataset, tokenizer)\n",
    "\n",
    "    # --- Load Base Model ---\n",
    "    # Must be loaded fresh for each new run\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16, # More efficient than \"auto\"\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    # Ensure model's pad_token_id is set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_type_id\n",
    "\n",
    "    # --- PEFT (LoRA) Configuration ---\n",
    "    # Pull hyperparameters directly from wandb.config\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"], # From your notebook\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(f\"--- Run: {run.name} ---\")\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    # Define a unique output dir for each run based on its wandb name\n",
    "    output_dir = os.path.join(WORKING_DIR, \"wandb-sweeps\", run.name)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        \n",
    "        # --- Hyperparameters from wandb config ---\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        learning_rate=config.learning_rate,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        \n",
    "        # --- Other Training Params ---\n",
    "        per_device_eval_batch_size=config.per_device_train_batch_size * 2, # Eval can use larger batch\n",
    "        gradient_accumulation_steps=2, # Accumulate gradients to simulate larger batch size\n",
    "        optim=\"paged_adamw_8bit\",    # Efficient optimizer for LoRA\n",
    "        \n",
    "        logging_steps=50,  # Log more frequently\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\", # This is what the sweep will track\n",
    "        greater_is_better=False,\n",
    "\n",
    "        report_to=\"wandb\",  \n",
    "        \n",
    "        bf16=torch.cuda.is_bf16_supported(), # Use bfloat16 if available\n",
    "        fp16=False, # Mutually exclusive with bf16\n",
    "    )\n",
    "\n",
    "    # --- Trainer ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    try:\n",
    "        print(f\"Starting training for run {run.name}...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the best model from this run\n",
    "        best_model_path = os.path.join(output_dir, \"best_model\")\n",
    "        trainer.model.save_pretrained(best_model_path)\n",
    "        print(f\"Best model for run {run.name} saved to {best_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training for run {run.name}: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # 7. Finish the wandb run\n",
    "        print(f\"Finishing run {run.name}.\")\n",
    "        run.finish()\n",
    "\n",
    "# --- Main execution to set up and run the sweep ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Define the Sweep Configuration\n",
    "    # This dictionary tells wandb what hyperparameters to try\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',  # Use Bayesian optimization\n",
    "        'metric': {\n",
    "            'name': 'eval/loss', # This is what Trainer logs\n",
    "            'goal': 'minimize'     # We want to minimize the validation loss\n",
    "        },\n",
    "        'parameters': {\n",
    "            'learning_rate': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-5,  # Start from 0.00001\n",
    "                'max': 5e-4   # Go up to 0.0005\n",
    "            },\n",
    "            'lora_r': {\n",
    "                'values': [16, 32, 64] # Your notebook used 64\n",
    "            },\n",
    "            'lora_alpha': {\n",
    "                'values': [32, 64, 128] # Often 1x or 2x of r\n",
    "            },\n",
    "            'lora_dropout': {\n",
    "                'values': [0.05, 0.1]\n",
    "            },\n",
    "            'per_device_train_batch_size': {\n",
    "                'values': [4, 8] # Adjust based on your GPU memory\n",
    "            },\n",
    "            'num_train_epochs': {\n",
    "                'values': [3, 5] # Your notebook planned 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 2. Initialize the Sweep\n",
    "    # This tells wandb to create the sweep project\n",
    "    # You only need to run this line ONCE to create the sweep.\n",
    "    try:\n",
    "        sweep_id = wandb.sweep(sweep_config, project=os.environ[\"WANDB_PROJECT\"])\n",
    "        print(f\"Sweep created successfully. Sweep ID: {sweep_id}\")\n",
    "        \n",
    "        # 3. Start the agent\n",
    "        # This will run 10 trials sequentially on this machine.\n",
    "        # The agent will ask the wandb server for hyperparameters,\n",
    "        # then call the `train` function with them.\n",
    "        print(\"Starting wandb agent to run 10 trials...\")\n",
    "        wandb.agent(sweep_id, function=train, count=10)\n",
    "        \n",
    "        print(\"\\n--- Sweep Finished ---\")\n",
    "        print(f\"View all results at: https://wandb.ai/{wandb.api.default_entity}/{os.environ['WANDB_PROJECT']}/sweeps/{sweep_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up or running sweep: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSweep interrupted by user. Exiting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
