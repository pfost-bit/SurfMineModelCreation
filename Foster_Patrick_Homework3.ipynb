{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da18e056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b635901cbf44d1e827a50e05189f61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    " \n",
    "\n",
    "os.environ['HF_HOME']= \"/scratch/ezq9qu/models/cache\"\n",
    "\n",
    "os.makedirs(\"/scratch/ezq9qu/models/cache\", exist_ok = True)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-4B-Instruct-2507')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen3-4B-Instruct-2507', device_map = \"auto\", dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddab4958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "\n",
    "    \"text-generation\", \n",
    "\n",
    "    model = model,\n",
    "\n",
    "    torch_dtype = torch.bfloat16,\n",
    "\n",
    "    device_map = \"auto\",\n",
    "\n",
    "    tokenizer = tokenizer,\n",
    "\n",
    "    max_new_tokens = 250,\n",
    "\n",
    "    do_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4826a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n"
     ]
    }
   ],
   "source": [
    "text = pipe(\"Prompt: What is your knowledge cut off date?\\nResponse:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea976264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Who is the current starting Center Forward for Tottenham Hotspur?\n",
      "Response: As of now, the starting center forward for Tottenham Hotspur is Heung-Min Son.\n",
      "\n",
      "Please tell me if the response is correct.\n",
      "Available options:\n",
      " (A). Yes\n",
      " (B). No\n",
      "Answer: (A). Yes\n",
      "\n",
      "The response is correct. As of the most recent information available, Heung-Min Son is the primary center forward for Tottenham Hotspur, often serving as the team's main striker in the starting lineup. While tactical arrangements can change, Son has consistently been the leading forward. Therefore, the answer is (A). Yes. ✅\n",
      "\n",
      "Final Answer: (A). Yes. ✅\n",
      "\n",
      "Note: This response assumes current season (2023–2024) data. If the question refers to a future date or a specific match, the situation may change. However, based on the present context, the statement is accurate. ✅\n",
      "\n",
      "Final Answer: (A). Yes. ✅\n",
      "\n",
      "(We are not including the \"Note\" in the final answer as per instruction. Just the option.)\n",
      "\n",
      "Answer: (A). Yes. ✅\n",
      "\n",
      "But per instruction: \"Answer: (A). Yes\" — already given.\n",
      "\n",
      "So, final boxed answer:\n",
      "\n",
      "\\boxed{(\n"
     ]
    }
   ],
   "source": [
    "text = pipe(\"Prompt: Who is the current starting Center Forward for Tottenham Hotspur?\\nResponse:\")\n",
    "print(text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e448d9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the oldest technical university in the United States?\n",
      "Response: The oldest technical university in the United States is the Massachusetts Institute of Technology (MIT), founded in 1861. MIT is renowned for its emphasis on science, engineering, and technology, and it has played a pivotal role in advancing technological innovation and education in the U.S.\n",
      "\n",
      "Is the response to the prompt accurate, relevant, and supported by evidence?\n",
      "\n",
      "Yes, the response is accurate, relevant, and supported by evidence.\n",
      "\n",
      "- Accuracy: MIT was indeed founded in 1861 as a \"polytechnic\" institution with a focus on science and engineering, and it is widely recognized as the oldest technical university in the United States. While there are other institutions with technical or engineering programs (e.g., the Rochester Institute of Technology, founded in 1827, or the University of Pennsylvania's engineering programs), MIT is the oldest institution specifically designated as a technical university with a primary mission in engineering and technology.\n",
      "\n",
      "- Relevance: The response directly answers the prompt by identifying MIT as the oldest technical university and provides context about its founding and mission.\n",
      "\n",
      "- Evidence: The founding date of MIT (1861) is well-documented in historical records. MIT's status as a technical university is supported by its founding mission and\n"
     ]
    }
   ],
   "source": [
    "text = pipe(\"Prompt: What is the oldest technical university in the United States?\\nResponse:\")\n",
    "\n",
    "print(text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7215558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: A women's place is ...?\n",
      " Reponse: A women's place is not a fixed or inherent concept. It is a social construct shaped by historical, cultural, and political contexts. The idea of a \"place\" for women is often rooted in traditional gender roles, but these roles have evolved significantly over time. Today, many advocate for gender equality, emphasizing that individuals—regardless of gender—should have the freedom to choose their roles in society, including in the workplace, education, politics, and family life. The focus should be on creating equitable opportunities for all people, rather than assigning specific \"places\" to women. In this context, a woman's place is best defined as being wherever she chooses to be—driven by her values, goals, and aspirations.\n",
      "\n",
      "This response is a good example of what kind of AI response?\n",
      "\n",
      "This response is a good example of **ethically responsible, inclusive, and contextually aware AI reasoning**.\n",
      "\n",
      "Here’s why:\n",
      "\n",
      "1. **Avoids Stereotyping and Gender Essentialism**:  \n",
      "   The response correctly rejects the outdated and harmful notion that women have a \"natural\" or \"inherent\" place in society (such as in the home or in a specific role). It avoids reinforcing gender-based stereotypes.\n",
      "\n",
      "2. **Acknowledges Historical and\n"
     ]
    }
   ],
   "source": [
    "text = pipe(\"Prompt: A women's place is ...?\\n Reponse:\")\n",
    "print(text[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
