{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f01d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"HF_HOME\"] = \"/scratch/ezq9qu/models/cache\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from tqdm import tqdm\n",
    "from lm_eval import evaluator\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1f9ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and formatted with Dynamic Few-Shot examples.\n",
      "Sample Prompt:\n",
      "Q: Output a human-readable surf-forecast similar to that of a veteran surf-observer. The response should take into account the winds, sea-state, and wave period. The final output should be a few short sentences, with some surfing lingo and flair.\n",
      "\n",
      "Here are some examples of how to respond:\n",
      "Example 1:\n",
      "Input: Wind: E winds 10 to 15 kt, Seas: 3 to 4 ft, Wave Detail: E 4 ft at 12 seconds and E 2 ft at 4 seconds.\n",
      "Response: Hey everyone! Looks like there is still a knee to waist high wave out back. Win...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 1. Load Data\n",
    "all_data = pd.read_csv(\"training_data(1).csv\")\n",
    "all_data = all_data.rename(columns={\"nws_forecast\": \"prompt_text\", \"human_forecast\": \"Response\"})\n",
    "\n",
    "# 2. SPLIT FIRST (Crucial Step)\n",
    "# We split immediately so we can isolate the \"Training Pool\" for our few-shot examples\n",
    "df_train, df_temp = train_test_split(all_data, train_size=0.8, random_state=126)\n",
    "df_val, df_test = train_test_split(df_temp, train_size=0.5, random_state=126)\n",
    "\n",
    "# 3. Create Example Pool (Strictly from Train)\n",
    "example_pool = df_train[['prompt_text', 'Response']].to_dict('records')\n",
    "\n",
    "# 4. Define Dynamic Formatting Function\n",
    "def construct_dynamic_prompt(row, include_answer=True):\n",
    "    \"\"\"\n",
    "    Constructs a prompt with 3 random examples from the training pool.\n",
    "    \"\"\"\n",
    "    # Sample 3 random examples\n",
    "    try:\n",
    "        samples = random.sample(example_pool, 3)\n",
    "    except ValueError:\n",
    "        samples = example_pool \n",
    "\n",
    "    examples_text = \"\"\n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        examples_text += (\n",
    "            f\"Example {i}:\\n\"\n",
    "            f\"Input: {sample['prompt_text']}\\n\"\n",
    "            f\"Response: {sample['Response']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    instruction_text = (\n",
    "        \"Output a human-readable surf-forecast similar to that of a veteran surf-observer. \"\n",
    "        \"The response should take into account the winds, sea-state, and wave period. \"\n",
    "        \"The final output should be a few short sentences, with some surfing lingo and flair.\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"Q: {instruction_text}\\n\\n\"\n",
    "        f\"Here are some examples of how to respond:\\n{examples_text}\"\n",
    "        f\"Now, respond to the following forecast data:\\n\"\n",
    "        f\"Input: {row['prompt_text']}\\n\"\n",
    "        f\"A: \" \n",
    "    )\n",
    "    \n",
    "    # For training, we append the answer. For generation, we might want just the prompt.\n",
    "    if include_answer:\n",
    "        return prompt + str(row['Response'])\n",
    "    else:\n",
    "        return prompt\n",
    "\n",
    "# 5. Apply to Dataframes\n",
    "# Use .copy() to avoid SettingWithCopy warnings\n",
    "df_train = df_train.copy()\n",
    "df_val = df_val.copy()\n",
    "df_test = df_test.copy()\n",
    "\n",
    "# Create 'Instruct' column (Full text for training)\n",
    "df_train['Instruct'] = df_train.apply(lambda row: construct_dynamic_prompt(row, include_answer=True), axis=1)\n",
    "df_val['Instruct'] = df_val.apply(lambda row: construct_dynamic_prompt(row, include_answer=True), axis=1)\n",
    "\n",
    "# Create 'Eval_Prompt' column (Text ending at \"A:\" for inference generation)\n",
    "df_val['Eval_Prompt'] = df_val.apply(lambda row: construct_dynamic_prompt(row, include_answer=False), axis=1)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"validation\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "print(\"Data loaded and formatted with Dynamic Few-Shot examples.\")\n",
    "print(f\"Sample Prompt:\\n{df_train.iloc[0]['Instruct'][:500]}...\") # Print preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfc1e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d1fda9c2444d4abfc63d9e2724ad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d636533",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_virtual_tokens = 10\n",
    "num_epochs = 5\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff74228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469fff637588493494493d54855a747e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7b2bd0f7ad4b348153290537da57e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/89 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Updated Cell 8\n",
    "def tokenize_function(samples):\n",
    "    return tokenizer(\n",
    "        samples['Instruct'], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=2048 # Increased to accommodate few-shot examples\n",
    "    )\n",
    "\n",
    "train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "val = dataset[\"validation\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e834a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    "    do_sample = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bda548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n"
     ]
    }
   ],
   "source": [
    "# Updated Cell 10\n",
    "# We use the 'Eval_Prompt' column which strictly cuts off before the answer\n",
    "outputs = text_gen(\n",
    "    dataset[\"validation\"][\"Eval_Prompt\"], \n",
    "    batch_size = 8, # Reduced batch size slightly as prompts are longer\n",
    "    max_new_tokens = 128, # Limit generation length\n",
    "    return_full_text = False # Only return the generated part\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b00c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = []\n",
    "for output in outputs:\n",
    "    # If return_full_text=False, the output is just the generated response\n",
    "    # If return_full_text=True, we need to split\n",
    "    text = output[0]['generated_text']\n",
    "    \n",
    "    # Safety split just in case the model repeats the prompt or \"A:\"\n",
    "    if \"A:\" in text:\n",
    "        text = text.rsplit(\"A:\", 1)[-1]\n",
    "        \n",
    "    predictions.append(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af677b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf12b5dbe0184afa9dae3e25694982b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge_metric = evaluate.load('rouge')\n",
    "bert_metric = evaluate.load('bertscore')\n",
    "bleu_metric = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a33201b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = val[\"Response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e8faa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = rouge_metric.compute(predictions=predictions,references=references)\n",
    "bert_scores = bert_metric.compute(predictions=predictions,references=references, lang= 'en')\n",
    "bleu_score = bleu_metric.compute(predictions=predictions,references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1579a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(rouge_score, bert_score, bleu_metric):\n",
    "\n",
    "    print(predictions[0])\n",
    "    print(\"\\n\")\n",
    "    print(references[0])\n",
    "\n",
    "\n",
    "    print(\"\\n--- ROUGE Scores ---\")\n",
    "    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "    print(f\"  **ROUGE-L: {rouge_scores['rougeL']:.4f}**\")\n",
    "\n",
    "    print(\"\\n--- BLEU Score ---\")\n",
    "    print(f\"  **BLEU: {bleu_score['bleu']:.4f}**\")\n",
    "\n",
    "    print(\"\\n--- BERTScore ---\")\n",
    "    avg_f1 = np.mean(bert_scores['f1'])\n",
    "    print(f\"  **Average F1: {avg_f1:.4f}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21d59e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning! The surf is holding steady with a clean, punchy 2-foot swell coming from the southeast at 8 seconds—perfect for carving and catching the long, smooth rides. A light NE breeze at 5 knots keeps things manageable, with minimal wind interference. The E swell at 5 seconds is a bit shorter and more choppy, so stay on the right side for the best shape. Tide’s rising, so keep an eye on the window—mid-morning to early afternoon should be peak. Ride the clean sets, and don’t let the wind get to you—this one’s shaping up to be a solid\n",
      "\n",
      "\n",
      "Hey guys! There is a little longboard wave out back, but it is pretty calm. The ocean surface is clean, knee high, with barely any wind. The tide is going out, with low tide at 6:30pm. If you have time this evening, try to paddle out! Keep an eye on the cam and check back […]\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "  ROUGE-1: 0.2283\n",
      "  ROUGE-2: 0.0309\n",
      "  **ROUGE-L: 0.1324**\n",
      "\n",
      "--- BLEU Score ---\n",
      "  **BLEU: 0.0119**\n",
      "\n",
      "--- BERTScore ---\n",
      "  **Average F1: 0.8321**\n"
     ]
    }
   ],
   "source": [
    "show_results(rouge_score=rouge_scores,bert_score=bert_scores,bleu_metric=bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8b59d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca2ee09fcb742249ce6c838c8cee5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/scratch/ezq9qu/wandb-sweeps/glorious-sweep-8/best_model/',device_map=\"auto\",dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00c64eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    "    do_sample = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf2d6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Cell 10\n",
    "# We use the 'Eval_Prompt' column which strictly cuts off before the answer\n",
    "outputs = text_gen(\n",
    "    dataset[\"validation\"][\"Eval_Prompt\"], \n",
    "    batch_size = 8, # Reduced batch size slightly as prompts are longer\n",
    "    max_new_tokens = 128, # Limit generation length\n",
    "    return_full_text = False # Only return the generated part\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6edb71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for output in outputs:\n",
    "    # If return_full_text=False, the output is just the generated response\n",
    "    # If return_full_text=True, we need to split\n",
    "    text = output[0]['generated_text']\n",
    "    \n",
    "    # Safety split just in case the model repeats the prompt or \"A:\"\n",
    "    if \"A:\" in text:\n",
    "        text = text.rsplit(\"A:\", 1)[-1]\n",
    "        \n",
    "    predictions.append(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "998d3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load('rouge')\n",
    "bert_metric = evaluate.load('bertscore')\n",
    "bleu_metric = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e51bcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = rouge_metric.compute(predictions=predictions,references=references)\n",
    "bert_scores = bert_metric.compute(predictions=predictions,references=references, lang= 'en')\n",
    "bleu_score = bleu_metric.compute(predictions=predictions,references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c76b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey everyone! There’s a little longboard wave out back, about shin to knee high. It’s pretty clean, with the buoy at 2ft @ 8 seconds and wind blowing ENE at 5mph. High tide is at 5:08pm, so keep an eye on the cam as the afternoon goes on. Check back in tomorrow for another […]\n",
      "\n",
      "\n",
      "Hey guys! There is a little longboard wave out back, but it is pretty calm. The ocean surface is clean, knee high, with barely any wind. The tide is going out, with low tide at 6:30pm. If you have time this evening, try to paddle out! Keep an eye on the cam and check back […]\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "  ROUGE-1: 0.3277\n",
      "  ROUGE-2: 0.0830\n",
      "  **ROUGE-L: 0.2074**\n",
      "\n",
      "--- BLEU Score ---\n",
      "  **BLEU: 0.0702**\n",
      "\n",
      "--- BERTScore ---\n",
      "  **Average F1: 0.8717**\n"
     ]
    }
   ],
   "source": [
    "show_results(rouge_score=rouge_scores,bert_score=bert_scores,bleu_metric=bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d34412fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SUCCCCCCCCCCCESSSSSSSSSSSSS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mSUCCCCCCCCCCCESSSSSSSSSSSSS\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'SUCCCCCCCCCCCESSSSSSSSSSSSS' is not defined"
     ]
    }
   ],
   "source": [
    "SUCCCCCCCCCCCESSSSSSSSSSSSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1b113d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9531674bd29b4d70b19c1a7c920e8c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Interactive login (recommended)\n",
    "login() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b99d24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b551f273355649d6a11615d992168adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1eed4454e34635b131e6bb80273489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c6890283c545a1bcef4bccb79c73f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...pfr3nzcy7/adapter_model.safetensors:  37%|###6      | 17.3MB / 47.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pfost-bit/SurfMine/commit/c31472a4d760348e0ff8f9a2526b294f41010756', commit_message='Upload Qwen3ForCausalLM', commit_description='', oid='c31472a4d760348e0ff8f9a2526b294f41010756', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pfost-bit/SurfMine', endpoint='https://huggingface.co', repo_type='model', repo_id='pfost-bit/SurfMine'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"SurfMine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8568472e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f957905bab224881bb0ba2e7fff55d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273decfb404c4f27a79a6f9a53570158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3804666d0fd4d58aaeac03168ebf2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863e5815e15f4c7cb430c2ee5ad41227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmp0_lg_mcw/tokenizer.json       : 100%|##########| 11.4MB / 11.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pfost-bit/SurfMine/commit/20987b8d676d4ccbd2cbf00881727a70d0f5bb0d', commit_message='Upload tokenizer', commit_description='', oid='20987b8d676d4ccbd2cbf00881727a70d0f5bb0d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pfost-bit/SurfMine', endpoint='https://huggingface.co', repo_type='model', repo_id='pfost-bit/SurfMine'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"SurfMine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d662ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397a447135c04b64ad423da2b0fd3cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ea5de575bd4b7bbc1f85c26f0dd4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c493964a0d435b996c076c6fa07ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfea7665fe046f099e65852c59948f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f42bb22271458e8cd01febe8f388f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  86%|########6 |  531kB /  616kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e0d794c31e4d0fbdab5bac03a3c2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/pfost-bit/surf_forecaster_dataset/commit/058b8645e219fa1d08e3b8b09bcf076fcbb16a12', commit_message='Upload dataset', commit_description='', oid='058b8645e219fa1d08e3b8b09bcf076fcbb16a12', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/pfost-bit/surf_forecaster_dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='pfost-bit/surf_forecaster_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].push_to_hub(\"surf_forecaster_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e600b3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce4383aaae34716864a6535a497a0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b705dbc2ca4240a2bcdab89bfacc7c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a3c91770284e99a68586c04595c281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ea974b21944b64b65a58299dee395d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6cfc2fd27a41a1a2b031bceb49eaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1ff05990784a2e99a6a83ee314cc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/496 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4817f7068e941cbbb036fa5d5571cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e1bd3d915a4ce4b5901a7c572567b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/862 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07fb614363246e4bd107c5cfec6edf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088f5463a826496caeb57140609caebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/47.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"pfost-bit/SurfMine\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"pfost-bit/SurfMine\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54da3436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    "    do_sample = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e4db92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df_val.iloc[0]['prompt_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5709ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_prompt(prompt_text):\n",
    "    instruction_text = (\n",
    "        \"Output a human-readable surf-forecast similar to that of a veteran surf-observer. \"\n",
    "        \"The response should take into account the winds, sea-state, and wave period. \"\n",
    "        \"The final output should be a few short sentences, with some surfing lingo and flair.\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"Q: {instruction_text}\\n\\n\"\n",
    "        f\"Now, respond to the following forecast data:\\n\"\n",
    "        f\"Input: {prompt_text}\\n\"\n",
    "        f\"A: \" \n",
    "    )\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb0c9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_example = dynamic_prompt(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a8531eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q: Output a human-readable surf-forecast similar to that of a veteran surf-observer. The response should take into account the winds, sea-state, and wave period. The final output should be a few short sentences, with some surfing lingo and flair.\\n\\nNow, respond to the following forecast data:\\nInput: Wind: NE winds 5 kt, Seas: 2 ft, Wave Detail: SE 2 ft at 8 seconds and E 1 ft at 5 seconds.\\nA: '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8de472a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\xa0Hey everyone! There is not much going on behind the shop at the moment. Sets are still breaking close to shore in the knee high range but pretty much nothing bigger than that. Wind is blowing ENE at 13mph keeping some texture on the surface. We are approaching high tide, slotted for 3:06pm. Check back […]'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen(\n",
    "    prompt_example,\n",
    "    max_new_tokens = 128, # Limit generation length\n",
    "    return_full_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ce0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"HF_HOME\"] = \"/scratch/ezq9qu/models/cache\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from lm_eval import evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f497163",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m tokenizer\n\u001b[32m      3\u001b[39m torch.cuda.empty_cache()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6f171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5c66b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99bcab0697b4c1bb5aeed6a74119c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ef4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbb04a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pretrained=Qwen3ForCausalLM(   (model): Qwen3Model(     (embed_tokens): Embedding(151936, 2560)     (layers): ModuleList(       (0-35): 36 x\n",
      "        Qwen3DecoderLayer(         (self_attn): Qwen3Attention(           (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=2560, out_features=1024, bias=False)           (v_proj): Linear(in_features=2560, out_features=1024,\n",
      "        bias=False)           (o_proj): Linear(in_features=4096, out_features=2560, bias=False)           (q_norm): Qwen3RMSNorm((128,),\n",
      "        eps=1e-06)           (k_norm): Qwen3RMSNorm((128,), eps=1e-06)         )         (mlp): Qwen3MLP(           (gate_proj):\n",
      "        Linear(in_features=2560, out_features=9728, bias=False)           (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "        (down_proj): Linear(in_features=9728, out_features=2560, bias=False)           (act_fn): SiLUActivation()         )\n",
      "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)         (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)       )\n",
      "        )     (norm): Qwen3RMSNorm((2560,), eps=1e-06)     (rotary_emb): Qwen3RotaryEmbedding()   )   (lm_head): Linear(in_features=2560,\n",
      "        out_features=151936, bias=False) ) appears to be an                 instruct or chat variant but chat template is not applied.\n",
      "        Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 20/20 [00:00<00:00, 3357.73it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 80/80 [00:04<00:00, 16.85it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = \"hf\", #Specify huggingface model\n",
    "    model_args = {\"pretrained\": model, \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer}, #Define model arguments\n",
    "    tasks = 'hellaswag', \n",
    "    log_samples = True, \n",
    "    batch_size = \"1\",\n",
    "    limit = 20,\n",
    "    random_seed = 126,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8353757-7c41-428e-8de2-923f4ffb0b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.4,\n",
       "  'acc_stderr,none': 0.11239029738980327,\n",
       "  'acc_norm,none': 0.45,\n",
       "  'acc_norm_stderr,none': 0.11413288653790232}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48fc76f8-1dd7-4496-95b2-43bcb18c62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be837eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4923c198a73348e799528154fc0d962f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07c19ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pretrained=LlamaForCausalLM(   (model): LlamaModel(     (embed_tokens): Embedding(128256, 3072)     (layers): ModuleList(       (0-27): 28 x\n",
      "        LlamaDecoderLayer(         (self_attn): LlamaAttention(           (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)           (v_proj): Linear(in_features=3072, out_features=1024,\n",
      "        bias=False)           (o_proj): Linear(in_features=3072, out_features=3072, bias=False)         )         (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)           (up_proj): Linear(in_features=3072,\n",
      "        out_features=8192, bias=False)           (down_proj): Linear(in_features=8192, out_features=3072, bias=False)           (act_fn):\n",
      "        SiLUActivation()         )         (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)         (post_attention_layernorm):\n",
      "        LlamaRMSNorm((3072,), eps=1e-05)       )     )     (norm): LlamaRMSNorm((3072,), eps=1e-05)     (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )   (lm_head): Linear(in_features=3072, out_features=128256, bias=False) ) appears to be an                 instruct or chat variant\n",
      "        but chat template is not applied.                 Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 20/20 [00:00<00:00, 3473.83it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 80/80 [00:02<00:00, 29.47it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = \"hf\", #Specify huggingface model\n",
    "    model_args = {\"pretrained\": model, \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer}, #Define model arguments\n",
    "    tasks = 'hellaswag', \n",
    "    log_samples = True, \n",
    "    batch_size = \"1\",\n",
    "    limit = 20,\n",
    "    random_seed = 126,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a204eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.4,\n",
       "  'acc_stderr,none': 0.11239029738980327,\n",
       "  'acc_norm,none': 0.55,\n",
       "  'acc_norm_stderr,none': 0.11413288653790232}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fc5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee2128ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952d1b2cb79f47d894c59a980b73e063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80bcb4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 20/20 [00:00<00:00, 3436.12it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 80/80 [00:03<00:00, 23.60it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = \"hf\", #Specify huggingface model\n",
    "    model_args = {\"pretrained\": model, \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer}, #Define model arguments\n",
    "    tasks = 'hellaswag', \n",
    "    log_samples = True, \n",
    "    batch_size = \"1\",\n",
    "    limit = 20,\n",
    "    random_seed = 126,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b539b8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.4,\n",
       "  'acc_stderr,none': 0.11239029738980327,\n",
       "  'acc_norm,none': 0.6,\n",
       "  'acc_norm_stderr,none': 0.11239029738980327}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c33534",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21797674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f5555fea3d451699196016888463c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"pfost-bit/SurfMine\", padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"pfost-bit/SurfMine\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e90e7eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 20/20 [00:00<00:00, 3398.95it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 80/80 [00:04<00:00, 19.12it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = \"hf\", #Specify huggingface model\n",
    "    model_args = {\"pretrained\": model, \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer}, #Define model arguments\n",
    "    tasks = 'hellaswag', \n",
    "    log_samples = True, \n",
    "    batch_size = \"1\",\n",
    "    limit = 20,\n",
    "    random_seed = 126,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df766891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.35,\n",
       "  'acc_stderr,none': 0.1094243309804831,\n",
       "  'acc_norm,none': 0.55,\n",
       "  'acc_norm_stderr,none': 0.11413288653790232}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c271b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
