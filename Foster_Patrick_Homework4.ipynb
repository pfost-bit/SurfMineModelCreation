{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd223ad5-55bb-4c0c-9696-fd2a0b4e7c9b",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "\n",
    "Your goal for this homework is to learn more about parameter importance in LLMs and get more comfortable with editing LLM parameters and using the Eleuther LM Evaluation Harness like we did in the Week 4 live session. This homework will guide you through several different experiments to get further insight into the importance of various components of the LLM architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9485c-cb0d-46b3-871d-17b170cd4477",
   "metadata": {},
   "source": [
    "## Step 1: Choose a model and load dependencies (10 points)\n",
    "In the code cell below:\n",
    "- Load a LLM other than a Llama 3 model\n",
    "- Import the necessary dependencies from transformers\n",
    "- Import torch and lm_eval\n",
    "- Print the model architecture so you can see what its components are named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6036e91f-3978-4170-9dc8-5c9cb189ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch/ezq9qu/models/cache'\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lm_eval\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b591eb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2394fe6943f451883ad66646a40aa80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "def load_model(name = 'Qwen/Qwen3-4B-Instruct-2507'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(name, device_map=\"auto\", dtype=torch.bfloat16)\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model, \n",
    "        dtype=torch.bfloat16, \n",
    "        device_map=\"auto\", \n",
    "        tokenizer = tokenizer, \n",
    "        max_new_tokens = 250,\n",
    "        do_sample = False\n",
    "    )\n",
    "    return model, pipe, tokenizer\n",
    "\n",
    "model, pipe, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c32075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76d11d-cf1c-4fb0-9360-c017575be933",
   "metadata": {},
   "source": [
    "## Step 2: Test baseline performance (18 points)\n",
    "Using the code from Week 4 as a template, choose a task from the Eleuther LM Evaluation Harness and test your model on it, logging the model samples like we did in class (9 points). This is a good opportunity to practice implementing one of the benchmarks you will use for your final project. Feel free to use the `limit` parameter to subset the task to a few questions so that the code runs more quickly. \n",
    "\n",
    "Like we did in class, print the model's accuracy on the task (3 points) and print two examples of questions and the model's associated responses (3 points each). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2557da4a-0a04-4b92-b2ef-74b6d0ce9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79641cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    }
   ],
   "source": [
    "lm_eval_model = HFLM(pretrained=model,\n",
    "tokenizer = tokenizer,\n",
    "dtype = \"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6200bc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 305.90it/s]\n",
      "Running generate_until requests: 100%|██████████| 3/3 [00:22<00:00,  7.44s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = lm_eval_model,\n",
    "    tasks = \"gsm8k\",\n",
    "    log_samples = True,\n",
    "    batch_size = 1,\n",
    "    limit=3, #set number of questions from benchmark\n",
    "    random_seed=126,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b7b04",
   "metadata": {},
   "source": [
    "We can see the results, by indexing to the results level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb17fde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gsm8k': {'alias': 'gsm8k',\n",
       "  'exact_match,strict-match': np.float64(1.0),\n",
       "  'exact_match_stderr,strict-match': 0.0,\n",
       "  'exact_match,flexible-extract': np.float64(0.6666666666666666),\n",
       "  'exact_match_stderr,flexible-extract': 0.33333333333333337}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd6f39",
   "metadata": {},
   "source": [
    "Here we see that the it scored a value of 1 with a standard error of 0 with the 3 question limit from my chosen test `gsm8k`. A perfect score! Now we can index and see a couple of question an response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf5f2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['samples']['gsm8k'][0]['doc']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580d60d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" Janet’s ducks lay 16 eggs per day. She eats 3 for breakfast and bakes 4 for muffins, so she uses 3 + 4 = 7 eggs. The remainder of the eggs is 16 - 7 = 9 eggs. She sells each egg for $2, so she makes 9 * 2 = <<9*2=18>>18 dollars every day at the farmers' market.\\n#### 18\\nAnswer: 18\"]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['samples']['gsm8k'][0]['resps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b403cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['samples']['gsm8k'][1]['doc']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6244a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' The robe takes 2 bolts of blue fiber and half that much white fiber, which is 2 / 2 = 1 bolt of white fiber. So, in total, it takes 2 + 1 = 3 bolts. #### 3\\nAnswer: #### 3']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['samples']['gsm8k'][1]['resps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce050eb5",
   "metadata": {},
   "source": [
    "Both of these are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15688e-d8eb-4b03-ad7b-2f6dfbc37ee1",
   "metadata": {},
   "source": [
    "## Step 3: Attention heads versus MLP layers (18 points)\n",
    "Choose a model layer (decoder block) you want to experiment with. Repeat Step 2 twice, once after deleting one of that layer's attention projections (7 points) and once after deleting one of that layer's MLP layers (7 points). Don't forget to reload the model after each experiment so that you are not deleting multiple model components (4 points)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "012746e0-c62f-4d0b-9762-0bf6a4e7eb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a7a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[3].self_attn.q_proj.weight.data*=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a2b110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -0., -0.,  ..., 0., -0., -0.],\n",
       "        [0., 0., -0.,  ..., 0., 0., -0.],\n",
       "        [0., 0., -0.,  ..., -0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -0., 0.],\n",
       "        [0., -0., 0.,  ..., -0., 0., 0.],\n",
       "        [-0., 0., -0.,  ..., -0., 0., -0.]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[3].self_attn.q_proj.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "743ee87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    }
   ],
   "source": [
    "model_attn = HFLM(pretrained=model, tokenizer=tokenizer, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf526596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 308.71it/s]\n",
      "Running generate_until requests: 100%|██████████| 3/3 [00:32<00:00, 10.69s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = model_attn,\n",
    "    tasks = \"gsm8k\",\n",
    "    log_samples = True,\n",
    "    batch_size = 1,\n",
    "    limit=3, #set number of questions from benchmark\n",
    "    random_seed=127,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585a4d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gsm8k': {'alias': 'gsm8k',\n",
       "  'exact_match,strict-match': np.float64(0.6666666666666666),\n",
       "  'exact_match_stderr,strict-match': 0.33333333333333337,\n",
       "  'exact_match,flexible-extract': np.float64(0.6666666666666666),\n",
       "  'exact_match_stderr,flexible-extract': 0.33333333333333337}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cceaec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['samples']['gsm8k'][0]['doc']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "310e7fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" Janet’s ducks lay 16 eggs per day. She eats 3 for breakfast and bakes 4 for her friends, so she has 16 - 3 - 4 = <<16-3-4=9>>9 eggs left. She sells these at $2 per egg, so she makes 9 * 2 = <<9*2=18>>18 dollars every day at the farmers' market.\\n#### 18\\nAnswer: 18\"]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['samples']['gsm8k'][0]['resps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dd56bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e53f3b94c684fb7adbc57560085983a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model, pipe, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d62a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[3].mlp.gate_proj.weight.data*=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df528ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., -0., -0.],\n",
       "        [-0., 0., 0.,  ..., -0., 0., -0.],\n",
       "        [-0., 0., 0.,  ..., -0., 0., -0.],\n",
       "        ...,\n",
       "        [-0., 0., -0.,  ..., -0., 0., -0.],\n",
       "        [0., 0., -0.,  ..., 0., 0., -0.],\n",
       "        [-0., 0., -0.,  ..., 0., 0., -0.]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[3].mlp.gate_proj.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "659109dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    }
   ],
   "source": [
    "model_mlp = HFLM(pretrained=model, tokenizer=tokenizer, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d2a1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 312.59it/s]\n",
      "Running generate_until requests: 100%|██████████| 3/3 [00:21<00:00,  7.28s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "    model = model_mlp,\n",
    "    tasks = \"gsm8k\",\n",
    "    log_samples = True,\n",
    "    batch_size = 1,\n",
    "    limit=3, #set number of questions from benchmark\n",
    "    random_seed=128,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9394cff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gsm8k': {'alias': 'gsm8k',\n",
       "  'exact_match,strict-match': np.float64(1.0),\n",
       "  'exact_match_stderr,strict-match': 0.0,\n",
       "  'exact_match,flexible-extract': np.float64(1.0),\n",
       "  'exact_match_stderr,flexible-extract': 0.0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ae389",
   "metadata": {},
   "source": [
    "Thats everything correct again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f8de8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" The ducks lay 16 eggs per day. Janet eats 3 eggs for breakfast and bakes 4 eggs into muffins, so she uses 3 + 4 = <<3+4=7>>7 eggs. The remainder of the eggs is 16 - 7 = <<16-7=9>>9 eggs. She sells each egg for $2, so she makes 9 * 2 = <<9*2=18>>18 dollars per day at the farmers' market.\\n#### 18\\nAnswer: 18\"]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['samples']['gsm8k'][0]['resps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46f493-332d-416f-93f6-58f0a3ba6330",
   "metadata": {},
   "source": [
    "## Step 4: Interpretation (18 points)\n",
    "In a markdown cell, interpret the results from your experiment in Step 3 in 2-3 sentences. In the layer you chose, do attention heads or MLP layers appear to be more important and how do you know (i.e., how do the results from the experiment back up that claim) (9 points)? Why might that be the case (9 points)? There aren't necessarily right or wrong answers- we are just looking for some critical thinking based on what you know about LLMs and how they work. Feel free to peruse outside sources if you are stumped, but just make sure you reference them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b31461",
   "metadata": {},
   "source": [
    "It seems as though the attention head is the more important of the two layers when deleting a small part of it. We see the results drop with the attention head weights to zero that we do not see in the results with the MLP. However this may be that the model architecture is sufficiently robust enough to have a complete failure of one small layer and still function correctly. The residuals might just route around the specific failure. In order to test this thery I ran another expirment, deleting all of the mlps in one layer, and increasing the limit to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d196b-3350-49c1-99f5-f55b7db5ed3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde6673857c24db9806254a633e8d750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new evaluation wrapper...\n",
      "Running evaluation with the entire MLP block disabled...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 320.30it/s]\n",
      "Running generate_until requests: 100%|██████████| 10/10 [01:12<00:00,  7.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gsm8k': {'alias': 'gsm8k',\n",
       "  'exact_match,strict-match': np.float64(0.9),\n",
       "  'exact_match_stderr,strict-match': 0.09999999999999999,\n",
       "  'exact_match,flexible-extract': np.float64(0.8),\n",
       "  'exact_match_stderr,flexible-extract': 0.13333333333333333}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model, pipe, tokenizer = load_model()\n",
    "\n",
    "\n",
    "model.model.layers[35].mlp.gate_proj.weight.data*=0\n",
    "model.model.layers[35].mlp.up_proj.weight.data*=0\n",
    "model.model.layers[35].mlp.down_proj.weight.data*=0\n",
    "\n",
    "\n",
    "print(\"Creating a new evaluation wrapper...\")\n",
    "model_mlp_off = HFLM(pretrained=model, tokenizer=tokenizer, dtype=\"bfloat16\")\n",
    "\n",
    "\n",
    "print(\"Running evaluation with the entire MLP block disabled...\")\n",
    "results = evaluator.simple_evaluate(\n",
    "    model = model_mlp_off,\n",
    "    tasks = \"gsm8k\",\n",
    "    log_samples = False,\n",
    "    batch_size = 1,\n",
    "    limit=10, \n",
    "    random_seed=131, \n",
    ")\n",
    "\n",
    "results['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f825b70",
   "metadata": {},
   "source": [
    "We do see some decrease in the way that the model anwers the question, however it is not as significant as deleting part of the attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7fdc8-b1f2-454f-98e0-92cd17e18b4e",
   "metadata": {},
   "source": [
    "## Step 5: First versus middle versus last model layer (18 points)\n",
    "Based on your results in Step 3 and interpretation in Step 4, choose the model component that you think is more important (MLP layer versus attention head projection). For that component, repeat Step 2 three times, once after deleting that component in the first model layer (5 points), once after deleting that component in a middle model layer (5 points), and once after deleting that component in the final model layer (5 points). As with Step 3, don't forget to reload the model after each experiment (3 points)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9eb6d73-182d-41ac-9e1c-acc76298f100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ea848c527148acae9bc1a54af2e786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 3/3 [00:00<00:00, 309.13it/s]\n",
      "Running generate_until requests: 100%|██████████| 3/3 [00:22<00:00,  7.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gsm8k': {'alias': 'gsm8k',\n",
       "  'exact_match,strict-match': np.float64(1.0),\n",
       "  'exact_match_stderr,strict-match': 0.0,\n",
       "  'exact_match,flexible-extract': np.float64(1.0),\n",
       "  'exact_match_stderr,flexible-extract': 0.0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, pipe, tokenizer = load_model()\n",
    "\n",
    "model.model.layers[1].self_attn.q_proj.weight.data*=0\n",
    "\n",
    "model_attention_first = HFLM(pretrained=model, tokenizer=tokenizer, dtype=\"bfloat16\")\n",
    "\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model = model_attention_first,\n",
    "    tasks = \"gsm8k\",\n",
    "    log_samples = False,\n",
    "    batch_size = 1,\n",
    "    limit=3, \n",
    "    random_seed=132, \n",
    ")\n",
    "\n",
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0087e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36965aa1ee84277817942a7e20c525f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 3/3 [00:00<00:00, 309.82it/s]\n",
      "Running generate_until requests: 100%|██████████| 3/3 [00:23<00:00,  7.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gsm8k': {'alias': 'gsm8k',\n",
       "  'exact_match,strict-match': np.float64(1.0),\n",
       "  'exact_match_stderr,strict-match': 0.0,\n",
       "  'exact_match,flexible-extract': np.float64(0.6666666666666666),\n",
       "  'exact_match_stderr,flexible-extract': 0.33333333333333337}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, pipe, tokenizer = load_model()\n",
    "\n",
    "model.model.layers[16].self_attn.q_proj.weight.data*=0\n",
    "\n",
    "model_attention_middle = HFLM(pretrained=model, tokenizer=tokenizer, dtype=\"bfloat16\")\n",
    "\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model = model_attention_middle,\n",
    "    tasks = \"gsm8k\",\n",
    "    log_samples = False,\n",
    "    batch_size = 1,\n",
    "    limit=3, \n",
    "    random_seed=133, \n",
    ")\n",
    "\n",
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4f4ea6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c94aa5cd8164d5690c293b48fee5f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|██████████| 3/3 [00:00<00:00, 312.41it/s]\n",
      "Running generate_until requests: 100%|██████████| 3/3 [00:18<00:00,  6.21s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gsm8k': {'alias': 'gsm8k',\n",
       "  'exact_match,strict-match': np.float64(0.6666666666666666),\n",
       "  'exact_match_stderr,strict-match': 0.33333333333333337,\n",
       "  'exact_match,flexible-extract': np.float64(1.0),\n",
       "  'exact_match_stderr,flexible-extract': 0.0}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, pipe, tokenizer = load_model()\n",
    "\n",
    "model.model.layers[34].self_attn.q_proj.weight.data*=0\n",
    "\n",
    "model_attention_last = HFLM(pretrained=model, tokenizer=tokenizer, dtype=\"bfloat16\")\n",
    "\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model = model_attention_last,\n",
    "    tasks = \"gsm8k\",\n",
    "    log_samples = False,\n",
    "    batch_size = 1,\n",
    "    limit=3, \n",
    "    random_seed=134, \n",
    ")\n",
    "\n",
    "results['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b377fe4-926e-4621-a31c-c91e130f6ecf",
   "metadata": {},
   "source": [
    "## Step 6: Interpretation (18 points)\n",
    "Similar to Step 4, interpret the results from your experiment in Step 5 in 2-3 sentences. In your model, does the first, middle or last layer appear to be more important and how do you know (i.e., how do the results from the experiment back up that claim) (9 points)? Why might that be the case (9 points)? There aren't necessarily right or wrong answers- we are just looking for some critical thinking based on what you know about LLMs and how they work. Feel free to peruse outside sources if you are stumped, but just make sure you reference them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e56b1",
   "metadata": {},
   "source": [
    "The self attention blocks seem to matter more the later the layers. By removing the first layer we do not see a drop in performance from the model. This is not the case when we remove the later layers. When we change the q_proj for the middle layer and the later layer we see a significant drop in performance from both. The two layers seem to do different things however, the exact_match (strict) scores higher with the middle layer removed and the lower with the last layer removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f52f61",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
